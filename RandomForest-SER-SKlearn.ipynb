{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing binary decision trees with real valued features\n",
    "**Multi class supported - Classes IDs are 0 through C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(10000, 784)\n",
      "(7000, 784)\n",
      "(3000, 784)\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../MNIST_data/\", one_hot=False)\n",
    "Xtrain = mnist.train.images\n",
    "Xtest = mnist.test.images\n",
    "ytrain = mnist.train.labels\n",
    "ytest = mnist.test.labels\n",
    "Xsource, Xtarget, ysource, ytarget = train_test_split(Xtest, ytest, test_size=0.3)\n",
    "print Xtrain.shape\n",
    "print Xtest.shape\n",
    "print Xsource.shape\n",
    "print Xtarget.shape\n",
    "XT1,XT2,yT1,yT2 = train_test_split(Xtarget,ytarget,test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code for converting a Scikit Learn tree to our dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_from_scikit_learn_to_dic_ite(node_index,is_leaves, children_left,children_right,feature,threshold,value,labels,C):\n",
    "        \n",
    "        a = is_leaves[0]\n",
    "        b = feature[0]\n",
    "        c = threshold[0]\n",
    "        if (a):\n",
    "            d = value[0]  #datapoints of each class in the node\n",
    "            d2 = np.squeeze(d/np.sum(d))\n",
    "            d3 = np.zeros(C)\n",
    "            d3[labels] = d2\n",
    "            e = labels[np.argmax(d2)]\n",
    "            return {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True,\n",
    "            'prediction': e,\n",
    "            'labels_distribution':d3}\n",
    "    \n",
    "        else:\n",
    "            left = children_left[0]-node_index[0]\n",
    "            if(left==-1):\n",
    "                left_tree = None\n",
    "            else:\n",
    "                left_tree = convert_from_scikit_learn_to_dic_ite(node_index[left:],is_leaves[left:], children_left[left:],children_right[left:],feature[left:],threshold[left:],value[left:],labels,C)\n",
    "            right = children_right[0]-node_index[0]\n",
    "            if(right==-1):\n",
    "                right_tree = None\n",
    "            else:\n",
    "                right_tree = convert_from_scikit_learn_to_dic_ite(node_index[right:],is_leaves[right:], children_left[right:],children_right[right:],feature[right:],threshold[right:],value[right:],labels,C)\n",
    "            return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': b,\n",
    "            'threshold'        : c,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree,\n",
    "            'labels_distribution': None}\n",
    "\n",
    "\n",
    "def convert_from_scikit_learn_to_dic(tree,labels,C):\n",
    "    # C is the size of the whole labels\n",
    "    # labels are the labels that are used in the this tree\n",
    "\n",
    "    n_nodes = tree.tree_.node_count\n",
    "    children_left = tree.tree_.children_left\n",
    "    children_right = tree.tree_.children_right\n",
    "    feature = tree.tree_.feature\n",
    "    threshold = tree.tree_.threshold\n",
    "    node_index = np.array(range(0,n_nodes))\n",
    "    Val = tree.tree_.value   #datapoints in node\n",
    "\n",
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "    node_depth = np.zeros(shape=n_nodes)\n",
    "    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "    stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "    while len(stack) > 0:\n",
    "        node_id, parent_depth = stack.pop()\n",
    "        node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "        if (children_left[node_id] != children_right[node_id]):\n",
    "            stack.append((children_left[node_id], parent_depth + 1))\n",
    "            stack.append((children_right[node_id], parent_depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "    \n",
    "    return convert_from_scikit_learn_to_dic_ite(node_index,is_leaves, children_left,children_right,feature,threshold,Val,labels,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of how to use the conversion with Scikit Learn Decision Tree and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forest_convert(estimator):\n",
    "    RF = []  #the new RF list\n",
    "    ntrees = estimator.n_estimators\n",
    "    for t in range(ntrees):\n",
    "        tree = estimator.estimators_[t] #Scikit learn tree\n",
    "        Newtree = convert_from_scikit_learn_to_dic(tree) #converts to dictionary and saves to list\n",
    "        RF.append(Newtree)\n",
    "    return RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forest functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forest_create(X,y,ntrees,nvarsample=None):#, min_node_size = 5, Nbins = 10,max_depth=20):\n",
    "    \n",
    "    if nvarsample == None:\n",
    "        nvarsample = (np.round(np.sqrt(X.shape[1]))).astype(int)\n",
    "        print 'Nfeatures = %s'%nvarsample\n",
    "    \n",
    "    #the number of classes is inferred from the data\n",
    "    C = len(np.unique(y))\n",
    "    \n",
    "    nptrain = X.shape[0] #how many datapoints each tree is trained (same size of X)\n",
    "    \n",
    "    RF = []\n",
    "    \n",
    "    #for loop creating and training each tree \n",
    "    #bootstrap X to train each tree\n",
    "    for t in range(ntrees):\n",
    "        print 'current trained tree = %s'%t\n",
    "        #create bootstrap training dataset for tree t\n",
    "        indbootstrap = np.random.choice(X.shape[0],nptrain)\n",
    "        Xtree = X[indbootstrap,:]\n",
    "        ytree = y[indbootstrap] \n",
    "        \n",
    "        #train the tree\n",
    "        estimator = DecisionTreeClassifier(max_leaf_nodes=20)\n",
    "        estimator = estimator.fit(Xtree, ytree)\n",
    "        Newtree = convert_from_scikit_learn_to_dic(estimator)\n",
    "        print count_leaves(Newtree)\n",
    "        print count_nodes(Newtree)\n",
    "        #Newtree = decision_tree_create(Xtree,ytree,nvarsample,C,min_node_size,Nbins,Verbose=False,max_depth = max_depth)\n",
    "        RF.append(Newtree)\n",
    "    \n",
    "    print 'Forest Trained!'\n",
    "    return RF\n",
    "    \n",
    "\n",
    "#outputs the posterior prob of each tree and the corresponding class\n",
    "def forest_posterior(RF,x):\n",
    "\n",
    "    T = len(RF)  #the number of trees \n",
    "\n",
    "    #infer the number of classes\n",
    "    P0 = classify(RF[0],x)\n",
    "    C = len(P0)\n",
    "    \n",
    "    Pt = np.zeros((T,C)) #matrix of posteriors from each tree (T x Nclasses)\n",
    "    Pt[0,:] = P0\n",
    "    for t in range(len(RF))[1:]:\n",
    "        Pt[t,:] = classify(RF[t],x) \n",
    "    return Pt\n",
    " \n",
    "    \n",
    "#classify input based on majority voting of each tree prediction\n",
    "def forest_classify_majority(RF,x):\n",
    "        Pt = forest_posterior(RF,x)\n",
    "        Yt = np.argmax(Pt,axis=1)         \n",
    "        C,unique_counts = np.unique(Yt,return_counts=True) #the id of classes and number of each\n",
    "        return C[np.argmax(unique_counts)]   \n",
    "    \n",
    "#classify input by averaging posteriors \n",
    "def forest_classify_ensemble(RF,x):\n",
    "    Pt = forest_posterior(RF,x)\n",
    "    Pforest = Pt.mean(axis=0)\n",
    "    ypred = np.argmax(Pt.mean(axis=0))\n",
    "    return ypred\n",
    "\n",
    "def evaluate_classification_error(RF, X, y, method = None):  \n",
    "    # Apply the forest_classify(RF, x) to each row in your data\n",
    "    if method == None:\n",
    "        ypred = map(lambda x: forest_classify_ensemble(RF,x), X)\n",
    "        # Once you've made the predictions, calculate the classification error and return it\n",
    "        mistakes = sum(ypred != y)\n",
    "        error = mistakes/len(y)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    \n",
    "    # Corner case: If labels_in_node is empty, return 0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0\n",
    "    \n",
    "    C,unique_counts = np.unique(labels_in_node,return_counts=True) #the id of classes and number of each\n",
    "    \n",
    "    return (len(labels_in_node) - unique_counts[np.argmax(unique_counts)])\n",
    "\n",
    "\n",
    "def reached_minimum_node_size(y, min_node_size):\n",
    "    # Return True if the number of data points is less than or equal to the minimum node size.\n",
    "    if y.shape[0] <= min_node_size:\n",
    "        #print y.shape[0]\n",
    "        return True\n",
    "\n",
    "    \n",
    "# X matrix of features (p datapoints x N features)\n",
    "# y vector of labels (p x 1)\n",
    "\n",
    "def best_splitting_feature(X, y, Nbins):\n",
    "        \n",
    "    best_feature = None # Keep track of the best feature \n",
    "    best_threshold = None\n",
    "    best_I = -1     # Keep track of the best info gain so far \n",
    "\n",
    "    #the number of data points in the parent node\n",
    "    num_data_points = y.shape[0]\n",
    "    \n",
    "    #the entropy of the parent node\n",
    "    Hy = entropy(y)\n",
    "    \n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in range(X.shape[1]):\n",
    "        \n",
    "        fvals = X[:,feature]\n",
    "        #fvals = np.sort(fvals)  #sorting the values\n",
    "        if num_data_points > Nbins:            \n",
    "            fvals = fvals[range(0,num_data_points,Nbins)]\n",
    "        \n",
    "        #loop through all values of current feature to find the best split\n",
    "        for threshold in fvals:\n",
    "\n",
    "            # The left split will have all data points where the feature value is smaller than threshold\n",
    "            ind_left = X[:,feature] < threshold\n",
    "             # The right split will have all data points where the feature value is larger or equal\n",
    "            ind_right = X[:,feature] >= threshold\n",
    "            \n",
    "            #compute info-gain for current feature and threshold split\n",
    "            I = infogain(Hy,len(y),y[ind_left],y[ind_right])\n",
    "            \n",
    "            # If this is the best error we have found so far, store the feature as best_feature\n",
    "            # the threshold as the best threshold and the error as best_error\n",
    "            if I > best_I:\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "                best_I = I\n",
    "        \n",
    "    return best_feature, best_threshold # Return the best feature and threshold\n",
    "\n",
    "\n",
    "def infogain(Hy,Nparent,yleft,yright):\n",
    "    \n",
    "    Nleft = len(yleft)\n",
    "    Nright = len(yright)\n",
    "    \n",
    "    #when one of the splits is empty returns I = 0\n",
    "    if Nleft ==0 or Nright == 0:\n",
    "        I = 0\n",
    "    else:\n",
    "        #compute information gain\n",
    "        I = Hy -( (Nleft/Nparent)*entropy(yleft) + (Nright/Nparent)*entropy(yright) )   \n",
    "\n",
    "    return I\n",
    "\n",
    "\n",
    "#entropy for multiple classes\n",
    "def entropy(y):\n",
    "    C,unique_counts = np.unique(y,return_counts=True) #the id of classes and number of each\n",
    "    Pc = unique_counts/len(y)\n",
    "    H = -(Pc*np.log(Pc)).sum()\n",
    "    return H    \n",
    "\n",
    "\n",
    "def create_leaf(target_values,C):\n",
    "\n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True,\n",
    "            'prediction': None,\n",
    "            'labels_distribution':None                       }   \n",
    "    \n",
    "    # Count the number of data points of each class in the leaf.\n",
    "    C_in_node,unique_counts = np.unique(target_values,return_counts=True) #the id of classes and number of each\n",
    "    leaf['prediction'] = C_in_node[np.argmax(unique_counts)]\n",
    "    \n",
    "    Classes = np.zeros(C)\n",
    "    Classes[C_in_node] = unique_counts/len(target_values)\n",
    "    leaf['labels_distribution'] = Classes\n",
    "    \n",
    "    # Return the leaf node        \n",
    "    return leaf \n",
    "\n",
    "\n",
    "def decision_tree_create(X, y, N_features_to_sample, C, min_node_size, Nbins, Verbose, current_depth = 0, max_depth = 10):\n",
    "    \n",
    "    #randomly sample a subset of features\n",
    "    Nfeatures = X.shape[1]\n",
    "    features = np.random.choice(Nfeatures, N_features_to_sample, replace=False)    \n",
    "    \n",
    "    #select only the features sampled for this run\n",
    "    Xcurrent = X[:,features]\n",
    "    target_values = y\n",
    "\n",
    "    if Verbose == True:\n",
    "        print \"--------------------------------------------------------------------\"\n",
    "        print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "        print \"Features selected = %s\" % features\n",
    "\n",
    "\n",
    "    # Stopping condition 1\n",
    "    # (Check if there are mistakes at current node, i.e. if the node is pure.)\n",
    "    if intermediate_node_num_mistakes(target_values) == 0:  \n",
    "        if Verbose == True:\n",
    "            print \"No Mistakes at current node - Stopping.\"     \n",
    "        # If not mistakes at current node, make current node a leaf node\n",
    "        return create_leaf(target_values,C)\n",
    "    \n",
    "    #Stopping condition 2: min node size reached\n",
    "    if reached_minimum_node_size(y, min_node_size):\n",
    "        if Verbose == True:\n",
    "            print \"Minimum node size reached - Stopping\"\n",
    "        return create_leaf(y,C)\n",
    "    \n",
    "    # Stopping condition 3: (limit tree depth)\n",
    "    if current_depth >= max_depth:  \n",
    "        if Verbose == True:\n",
    "            print \"Reached maximum depth. Stopping.\"\n",
    "        # If the max tree depth has been reached, make current node a leaf node\n",
    "        return create_leaf(target_values,C)\n",
    "\n",
    "    # Find the best splitting feature and its threshold\n",
    "    splitting_feature,splitting_thres = best_splitting_feature(Xcurrent,y,Nbins)\n",
    "    splitting_feature = features[splitting_feature]\n",
    "    \n",
    "    # Split on the best feature that we found. \n",
    "    ind_left = X[:,splitting_feature] < splitting_thres\n",
    "    left_split = X[ind_left,:]\n",
    "    y_left = y[ind_left]\n",
    "\n",
    "    ind_right = X[:,splitting_feature] >= splitting_thres\n",
    "    right_split = X[ind_right,:]\n",
    "    y_right = y[ind_right]\n",
    "\n",
    "    if Verbose == True:\n",
    "        print \"Split on feature %s. (%s, %s), Threshold = %s\" % (\\\n",
    "        splitting_feature, y_left.shape, y_right.shape, splitting_thres)\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(y_left) == len(y) or len(y_right) == len(y):\n",
    "        if Verbose == True: \n",
    "            print 'One split empty: Creating Leaf'          \n",
    "        return create_leaf(y,C)  \n",
    "        \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = decision_tree_create(left_split, y_left, N_features_to_sample, C, min_node_size, Nbins, Verbose, current_depth + 1, max_depth)        \n",
    "    right_tree = decision_tree_create(right_split, y_right, N_features_to_sample, C, min_node_size, Nbins, Verbose, current_depth + 1, max_depth)\n",
    "\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'threshold'        : splitting_thres,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree,\n",
    "            'labels_distribution': None \n",
    "            \n",
    "            }\n",
    "\n",
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])\n",
    "\n",
    "def count_leaves(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1 \n",
    "    return count_leaves(tree['left']) + count_leaves(tree['right'])\n",
    "\n",
    "def classify(tree, x):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        return tree['labels_distribution'] \n",
    "    else:\n",
    "        # split on feature.\n",
    "        val_split_feature = x[tree['splitting_feature']]\n",
    "        if val_split_feature < tree['threshold']:\n",
    "            return classify(tree['left'], x)\n",
    "        else:\n",
    "            return classify(tree['right'],x)\n",
    "        \n",
    "def evaluate_classification_error_tree(tree, X, y):\n",
    "    if type(y) == np.uint8:# or X.shape[0] < 2:   #if we have only 1 datapoint in X\n",
    "        P = classify(tree,X)\n",
    "        prediction = np.argmax(P)\n",
    "        error = int(prediction != y)\n",
    "    else:\n",
    "        # Apply the classify(tree, x) to each row in your data\n",
    "\n",
    "        P = map(lambda x: classify(tree,x), X)\n",
    "        P = np.asarray(P)\n",
    "        prediction = np.argmax(P,axis=1)\n",
    "        # Once you've made the predictions, calculate the classification error and return it\n",
    "        mistakes = sum(prediction != y)\n",
    "        error = mistakes/len(y)\n",
    "        \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SER Algorithm functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Refine the trained forest on target data using the SER algorithm\n",
    "#RF scikit learn\n",
    "\n",
    "def forest_SER(RF,XT,yT,C,max_depth=2):\n",
    "\n",
    "    nptrain = len(yT) #how many datapoints each tree is trained (same size of yT)\n",
    "    ntrees = len(RF)\n",
    "    RFnew = []\n",
    "\n",
    "    for t in range(ntrees):\n",
    "        print 'expanding/reducing tree = %s'%t\n",
    "        #Bootstrap XT1 and XT2\n",
    "        indbootstrap1 = np.random.choice(XT.shape[0],nptrain)\n",
    "        indbootstrap2 = np.random.choice(XT.shape[0],nptrain)\n",
    "        XT1 = XT[indbootstrap1,:]\n",
    "        XT2 = XT[indbootstrap2,:]\n",
    "        yT1 = yT[indbootstrap1]\n",
    "        yT2 = yT[indbootstrap2]\n",
    "\n",
    "        treeNew = expansion_reduction(RF[t],XT1,yT1,XT2,yT2,max_depth,C)\n",
    "        RFnew.append(treeNew)\n",
    "        \n",
    "    print 'Forest refined on target data!'\n",
    "    return RFnew\n",
    "\n",
    "\n",
    "# compute the path to the leaf followed by data point x  \n",
    "def datapath(tree, x, branch = 1):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        return branch \n",
    "    else:\n",
    "        # split on feature.\n",
    "        split_feature = tree['splitting_feature']\n",
    "        split_threshold = tree['threshold']\n",
    "\n",
    "        if x[split_feature] < split_threshold:\n",
    "            return datapath(tree['left'], x, 2*branch)\n",
    "        else:\n",
    "            return datapath(tree['right'],x, 2*branch+1)\n",
    "\n",
    "        \n",
    "def expansion_reduction(tree,XT1,yT1,XT2,yT2,max_depth=2,C=-1,min_node_size = 5):\n",
    "\n",
    "    #Tree = tree #a copy of the tree\n",
    "    \n",
    "    #finding the leaf where each target datapoint ends up\n",
    "    leavesData1 = map(lambda x: datapath(tree,x), XT1)\n",
    "    leavesData2 = map(lambda x: datapath(tree,x), XT2)\n",
    "            \n",
    "    Uleaves1 = np.unique(leavesData1)  #the path to each leaf followed by data1\n",
    "    Uleaves2 = np.unique(leavesData2)  #the path to each leaf followed by data2\n",
    "    Uleaves = list(set(Uleaves1) & set(Uleaves2)) #leaves reached by both data1 and data2\n",
    "            \n",
    "    #expanding each leaf on the 1st bootstrap replica of target data\n",
    "    for i in Uleaves:\n",
    "        ind_data1 = leavesData1==i #indices of datapoints for each leaf\n",
    " \n",
    "        Exp_tree = decision_tree_create(XT1[ind_data1,:],yT1[ind_data1],XT1.shape[1], C, min_node_size, Nbins = 10, Verbose = False, current_depth=1,max_depth=max_depth)\n",
    "\n",
    "        #Is this a good expansion?: computes classification error at each leaf for Data T2\n",
    "        ind_data2 = leavesData2==i\n",
    "        Err_leavesT2 = intermediate_node_num_mistakes(yT2[ind_data2])/len(yT2[ind_data2])\n",
    "\n",
    "        #error at the current subtree on Data T2\n",
    "        Err_subtreeT2 = evaluate_classification_error_tree(Exp_tree, XT2[ind_data2,:], yT2[ind_data2])\n",
    "        \n",
    "        #comparing the error of the subtree with that at the leaf node of the original tree\n",
    "        if Err_subtreeT2 < Err_leavesT2:\n",
    "            tree = mergetrees(tree,i,Exp_tree)\n",
    "            #print 'merging successful!'\n",
    "        else:\n",
    "            print 'no merging: discard subtree'\n",
    "    \n",
    "    return tree\n",
    "\n",
    "\n",
    "def expansion_reduction_SKL(tree,XT1,yT1,XT2,yT2,C):\n",
    "\n",
    "    \n",
    "    #finding the leaf where each target datapoint ends up\n",
    "    leavesData1 = map(lambda x: datapath(tree,x), XT1)\n",
    "    leavesData2 = map(lambda x: datapath(tree,x), XT2)\n",
    "            \n",
    "    Uleaves1 = np.unique(leavesData1)  #the path to each leaf followed by data1\n",
    "    Uleaves2 = np.unique(leavesData2)  #the path to each leaf followed by data2\n",
    "    Uleaves = list(set(Uleaves1) & set(Uleaves2)) #leaves reached by both data1 and data2\n",
    "            \n",
    "    #expanding each leaf on the 1st bootstrap replica of target data\n",
    "    for i in Uleaves:\n",
    "        \n",
    "        ind_data1 = leavesData1==i #indices of datapoints for each leaf\n",
    "        ind_data2 = leavesData2==i\n",
    "        \n",
    "        if len(ind_data1) < 2:  #do not expand if there is only 1 datapoint\n",
    "            continue\n",
    "            \n",
    "        estimator = DecisionTreeClassifier(max_features='sqrt') #use sqrt the number of feat.\n",
    "        estimator = estimator.fit(XT1[ind_data1,:],yT1[ind_data1]) \n",
    "        #print 'Nclasses ExpTree = %s'%np.unique(yT1[ind_data1]) #%estimator.tree_.n_classes\n",
    "        \n",
    "        Exp_tree = convert_from_scikit_learn_to_dic(estimator,np.unique(yT1[ind_data1]),C)\n",
    "\n",
    "        #Is this a good expansion?: computes classification error at each leaf for Data T2\n",
    "        Err_leavesT2 = intermediate_node_num_mistakes(yT2[ind_data2])/len(yT2[ind_data2])\n",
    "\n",
    "        #error at the current subtree on Data T2\n",
    "        Err_subtreeT2 = evaluate_classification_error_tree(Exp_tree, XT2[ind_data2,:], yT2[ind_data2])\n",
    "        #comparing the error of the subtree with that at the leaf node of the original tree\n",
    "        if Err_subtreeT2 < Err_leavesT2:\n",
    "            tree = mergetrees(tree,i,Exp_tree)\n",
    "            #print 'merging successful!'\n",
    "            #print 'subtree nodes = %s'%count_nodes(Exp_tree)\n",
    "\n",
    "        #else:\n",
    "            #print 'no merging: discard subtree'\n",
    "    \n",
    "    return tree\n",
    "\n",
    "\n",
    "\n",
    "def mergetrees(tree1,leafnr,tree2):\n",
    "    leafnrbin = bin(leafnr)[3:]  #path is from the 4th element of the binary on: 0 = go left, 1 = go right\n",
    "    path = ''\n",
    "    for i in range(len(leafnrbin)):\n",
    "        if leafnrbin[i] == '0':\n",
    "            path=path+str(\"['left']\")\n",
    "        else:\n",
    "            path=path+str(\"['right']\") \n",
    "    # print(path)\n",
    "    exec ('tree1'+path+\"['prediction']\"+'=None')\n",
    "    exec ('tree1'+path+\"['is_leaf']\"+'=False')\n",
    "    exec ('tree1'+path+\"['left']\"+\"=tree2['left']\")\n",
    "    exec ('tree1'+path+\"['right']\"+\"=tree2['right']\")\n",
    "    exec ('tree1'+path+\"['splitting_feature']\"+\"=tree2['splitting_feature']\")\n",
    "    exec ('tree1'+path+\"['threshold']\"+\"=tree2['threshold']\")\n",
    "    exec ('del(tree1'+path+\"['labels_distribution'])\")\n",
    "    return tree1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2826\n"
     ]
    }
   ],
   "source": [
    "estimator = DecisionTreeClassifier(max_features='sqrt',max_leaf_nodes=100,random_state=0)\n",
    "estimator = estimator.fit(Xsource, ysource)\n",
    "tree = convert_from_scikit_learn_to_dic(estimator,np.unique(ysource),10)\n",
    "err1 = evaluate_classification_error_tree(tree, Xtest, ytest)\n",
    "print err1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exptree = expansion_reduction_SKL(tree,XT1,yT1,XT2,yT2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.259\n"
     ]
    }
   ],
   "source": [
    "err2 = evaluate_classification_error_tree(exptree, Xtest, ytest)\n",
    "print err2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': True,\n",
       " 'labels_distribution': array([ 0.        ,  0.        ,  0.00938967,  0.87323944,  0.        ,\n",
       "         0.02347418,  0.        ,  0.00469484,  0.07042254,  0.01877934,  0.        ]),\n",
       " 'left': None,\n",
       " 'prediction': 3,\n",
       " 'right': None,\n",
       " 'splitting_feature': None}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree['left']['right']['left']['right']['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_classification_error_tree(exptree, X, y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "\n",
      "*** Profile printout saved to text file u'expansion_reduction_speed.txt'. \n"
     ]
    }
   ],
   "source": [
    "%lprun -s -f expansion_reduction -T expansion_reduction_speed.txt expansion_reduction(tree,XT1,yT1,XT2,yT2,max_depth=2,C=10,min_node_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expansion_reduction_SKL() got an unexpected keyword argument 'max_depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-24df4b4bf387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'lprun -s -f expansion_reduction_SKL -T expansion_reduction_SKL_speed.txt expansion_reduction_SKL(tree,XT1,yT1,XT2,yT2,max_depth=2,C=10,min_node_size = 5)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/luca/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2161\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2165\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luca/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2082\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2084\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luca/anaconda/lib/python2.7/site-packages/line_profiler.pyc\u001b[0m in \u001b[0;36mmagic_lprun\u001b[0;34m(self, parameter_s)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luca/anaconda/lib/python2.7/site-packages/line_profiler.pyc\u001b[0m in \u001b[0;36mrunctx\u001b[0;34m(self, cmd, globals, locals)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_by_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mexec_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_by_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luca/anaconda/lib/python2.7/site-packages/line_profiler.pyc\u001b[0m in \u001b[0;36mexec_\u001b[0;34m(_code_, _globs_, _locs_)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0m_locs_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0m_locs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_globs_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"exec _code_ in _globs_, _locs_\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# ============================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luca/anaconda/lib/python2.7/site-packages/line_profiler.pyc\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expansion_reduction_SKL() got an unexpected keyword argument 'max_depth'"
     ]
    }
   ],
   "source": [
    "%lprun -s -f expansion_reduction_SKL -T expansion_reduction_SKL_speed.txt expansion_reduction_SKL(tree,XT1,yT1,XT2,yT2,max_depth=2,C=10,min_node_size = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Scikit Learn + Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count_leaves' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f90e1cdaced5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXsource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mysource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtree1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_from_scikit_learn_to_dic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mcount_leaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mcount_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count_leaves' is not defined"
     ]
    }
   ],
   "source": [
    "estimator = DecisionTreeClassifier(max_leaf_nodes=10)\n",
    "estimator.fit(Xsource, ysource)\n",
    "tree1 = convert_from_scikit_learn_to_dic(estimator)\n",
    "print count_leaves(tree1)\n",
    "print count_nodes(tree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = RandomForestClassifier(n_estimators=20,criterion='entropy',min_samples_leaf=10)\n",
    "estimator = estimator.fit(Xsource, ysource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF = forest_convert(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree = estimator.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 20)\n"
     ]
    }
   ],
   "source": [
    "leaves = estimator.apply(Xtarget)\n",
    "print leaves.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expanding/reducing tree = 0\n",
      "[1 1 1 1 1 1 1 1 1 1 1]\n",
      "no merging: discard subtree\n",
      "[4 4 4 4 4 4 4]\n",
      "no merging: discard subtree\n",
      "[4 4 4 4]\n",
      "no merging: discard subtree\n",
      "[4 4 4 4 4 4 6]\n",
      "merging successful!\n",
      "[2 2 2 2 2]\n",
      "no merging: discard subtree\n",
      "[2 7 7 7 2 7 7 7 2 2 2 2 7 2 7 7 7 7]\n",
      "merging successful!\n",
      "[2 2 2 7 2 2 2 2 2 7 2 2 2 7 2 2]\n",
      "merging successful!\n",
      "[7 7 7 7 7]\n",
      "no merging: discard subtree\n",
      "[1 1 1 1 1 1 1]\n",
      "no merging: discard subtree\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "no merging: discard subtree\n",
      "[8 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "no merging: discard subtree\n",
      "[7 7 7 7 7 7 7 7 7 7]\n",
      "no merging: discard subtree\n",
      "[9 9 9 3 9 9 9 9 3 3 9 3 3 9 3 9 9 9 9]\n",
      "merging successful!\n",
      "[9 9 9]\n",
      "no merging: discard subtree\n",
      "[6 9 6 9 9 9 9 9]\n",
      "merging successful!\n",
      "[9 9 9 9 9]\n",
      "no merging: discard subtree\n",
      "[8 8 8 8]\n",
      "no merging: discard subtree\n",
      "[2]\n",
      "no merging: discard subtree\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "no merging: discard subtree\n",
      "[1 1]\n",
      "no merging: discard subtree\n",
      "[1 1]\n",
      "no merging: discard subtree\n",
      "[1 1 1 7 7 7 1 7 7 7 7 7]\n",
      "merging successful!\n",
      "[7 7 7 7 7 7]\n",
      "no merging: discard subtree\n",
      "[4 4 4]\n",
      "no merging: discard subtree\n",
      "[7 7 3 7 3 7 7 7 3 3 3 3 7 7 7 7]\n",
      "no merging: discard subtree\n",
      "[0]\n",
      "no merging: discard subtree\n",
      "[4 4 4 4 4 4 4 4]\n",
      "no merging: discard subtree\n",
      "[2 2 7 2]\n",
      "merging successful!\n",
      "[2 2 2 2]\n",
      "no merging: discard subtree\n",
      "[5 5 5 5 5 5 5 5 5 5 5]\n",
      "no merging: discard subtree\n",
      "[6 6 4 6 4 4 6 4 6 6 6]\n",
      "merging successful!\n",
      "[0 4 0 4 4 4 0 0 0 0 4 0 4 0]\n",
      "merging successful!\n",
      "[7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      "no merging: discard subtree\n",
      "[3 3]\n",
      "no merging: discard subtree\n",
      "[3 1 1 1 1 1 1 1 3 1 1 3 3 1]\n",
      "merging successful!\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "no merging: discard subtree\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "no merging: discard subtree\n",
      "[6 9 6 9 6 6 6 6 6]\n",
      "merging successful!\n",
      "[4 4 9 4 4 9 9 4 9 4 4 4 4 4 4 4]\n",
      "merging successful!\n",
      "[5 7 7 7 7 5 5 7 5 5 5 5 5 5 5]\n",
      "no merging: discard subtree\n",
      "[5 5 4 5 5 5 5]\n",
      "merging successful!\n",
      "[0 0 5 0 0 5 0 0 5 0 5 5 0 5 0 0 0 5 0 5 0 0 0]\n",
      "merging successful!\n",
      "[3 3]\n",
      "no merging: discard subtree\n",
      "[4 4 4 4 4 4]\n",
      "no merging: discard subtree\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "no merging: discard subtree\n",
      "[3 0 0 0 3 3 3 0 0 0 0 3]\n",
      "merging successful!\n",
      "[1 7 7 1]\n",
      "merging successful!\n",
      "[4 4 4 4 4 4]\n",
      "no merging: discard subtree\n",
      "[3 3 3 3]\n",
      "no merging: discard subtree\n",
      "[4 4]\n",
      "no merging: discard subtree\n",
      "[4 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\n",
      "merging successful!\n",
      "[2 2 7 2 7 2 7 2 7 7 2 7 7 7 2 7 7]\n",
      "merging successful!\n",
      "[7 2 7 7 2 2 2 2]\n",
      "merging successful!\n",
      "[0 0 4 0 4 4 0 4 0 0 0 0 4 0]\n",
      "no merging: discard subtree\n",
      "[6 6 6 6 6 6 6 6 6 6 6 5 6 6 6 6]\n",
      "merging successful!\n",
      "[0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "merging successful!\n",
      "[5 5 5]\n",
      "no merging: discard subtree\n",
      "[0 0 0 0 4 4 0 4 0 0 0 0 4 0 0 0]\n",
      "merging successful!\n",
      "[2 9 9 9 2 2 2 9 2 9]\n",
      "merging successful!\n",
      "[3 3 3 3 3 3 3]\n",
      "no merging: discard subtree\n",
      "[1 1 1 1]\n",
      "no merging: discard subtree\n",
      "[3 3 3 3]\n",
      "no merging: discard subtree\n",
      "[2 2 2 2 2]\n",
      "no merging: discard subtree\n",
      "[6 6 6 6]\n",
      "no merging: discard subtree\n",
      "[1 2 2 1 2 1 2]\n",
      "merging successful!\n",
      "[2 2 2 2 2 2 2 2 2]\n",
      "no merging: discard subtree\n",
      "[9 9]\n",
      "no merging: discard subtree\n",
      "[3 3 5 5 3 3 3 5 3 3 3 3 3 3 3 3 3 3 3 5 3 3 3 3 3 3 5 3 5 3 3 3 3 3 3]\n",
      "no merging: discard subtree\n",
      "[8 8 8 8 8]\n",
      "no merging: discard subtree\n",
      "[8 8 8 5 5 8]\n",
      "no merging: discard subtree\n",
      "[3 3 4 3 3 4 3 4 3]\n",
      "no merging: discard subtree\n",
      "[7 9 9 7 9 7 7 7 9 7 9 9 7]\n",
      "merging successful!\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "no merging: discard subtree\n",
      "[2 2 0 2 0 2]\n",
      "no merging: discard subtree\n",
      "[3 5 3 3 3 3 3 3]\n",
      "merging successful!\n",
      "[3 3 3 3 3 8 8 8 8 3 8 8 8 3 3 3 3 3 3]\n",
      "no merging: discard subtree\n",
      "[8]\n",
      "no merging: discard subtree\n",
      "[8 8 8 8]\n",
      "no merging: discard subtree\n",
      "[8 8 7 8 7 7 7]\n",
      "no merging: discard subtree\n",
      "[8 8 8]\n",
      "no merging: discard subtree\n",
      "[2 6 2 2 2 6 6]\n",
      "merging successful!\n",
      "[7 7 7 7 7 7 7 7 7 7]\n",
      "no merging: discard subtree\n",
      "[0 0 0 5 0 5 0 5]\n",
      "no merging: discard subtree\n",
      "[9 9 5 5 9 5 9 5 9 9 9]\n",
      "merging successful!\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "no merging: discard subtree\n",
      "[3 2 2 2 2 2]\n",
      "merging successful!\n",
      "[9 9 9 9]\n",
      "no merging: discard subtree\n",
      "[9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\n",
      "no merging: discard subtree\n",
      "[2 2 2 7 2 2 2 2 2 7 2 2]\n",
      "merging successful!\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "no merging: discard subtree\n",
      "[0 5 0 0 5 5 5 0 5 5 5 5]\n",
      "no merging: discard subtree\n",
      "[0 0 0 3 3 0 3]\n",
      "no merging: discard subtree\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b2b2ee373ce9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnewRF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest_SER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-a03dbed31eb6>\u001b[0m in \u001b[0;36mforest_SER\u001b[0;34m(RF, XT, yT, C, max_depth)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0myT2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindbootstrap2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtreeNew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpansion_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXT1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myT1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXT2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myT2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mRFnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreeNew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a03dbed31eb6>\u001b[0m in \u001b[0;36mexpansion_reduction\u001b[0;34m(tree, XT1, yT1, XT2, yT2, max_depth, C, min_node_size)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mExp_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXT1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_data1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myT1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_data1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXT1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVerbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m#Is this a good expansion?: computes classification error at each leaf for Data T2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-880844757597>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, C, min_node_size, Nbins, Verbose, current_depth, max_depth)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# Find the best splitting feature and its threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0msplitting_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplitting_thres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_splitting_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXcurrent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0msplitting_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplitting_feature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-880844757597>\u001b[0m in \u001b[0;36mbest_splitting_feature\u001b[0;34m(X, y, Nbins)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m#compute info-gain for current feature and threshold split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfogain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_left\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_right\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# If this is the best error we have found so far, store the feature as best_feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-880844757597>\u001b[0m in \u001b[0;36minfogain\u001b[0;34m(yparent, yleft, yright)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m#compute information gain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myparent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNleft\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mNparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myleft\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNright\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mNparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myright\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-880844757597>\u001b[0m in \u001b[0;36mentropy\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munique_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#the id of classes and number of each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mPc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_counts\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "newRF = forest_SER(RF,Xtarget,ytarget,C=10,max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a fake dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#artificial training dataset\n",
    "Xtrain = np.random.rand(1000,10)\n",
    "y = np.ones(1000).astype(int)\n",
    "y[(Xtrain[:,0] < 0.25)] = 0\n",
    "y[(Xtrain[:,1] > 0.55)] = 2\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\n",
    "estimator.fit(Xtrain, y)\n",
    "a = convert_from_scikit_learn_to_dic(estimator)\n",
    "classify(a,Xtrain[150,:])\n",
    "y[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A linearly separable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain = np.random.rand(1000,10)\n",
    "y = np.ones(1000).astype(int)\n",
    "y[(Xtrain[:,0]+Xtrain[:,1]+Xtrain[:,2] < 1)] = 0\n",
    "y[(Xtrain[:,0]+3*Xtrain[:,2] >= 1.5)] = 2\n",
    "y[(1.5*Xtrain[:,0]-2*Xtrain[:,1]+Xtrain[:,2] < 1)] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another fake dataset with more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 10)\n",
      "[0 1]\n",
      "[-1  1]\n"
     ]
    }
   ],
   "source": [
    "X,y = make_classification(n_samples=2000,n_features=10,n_informative=5,n_classes=2)\n",
    "print X.shape\n",
    "print np.unique(y) #original labels are 0 and 1\n",
    "ind = y==0\n",
    "y[ind] = -1\n",
    "print np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split + Split test data into target1 (for expansion) and target2 (for reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can train on our version of random forest to compare the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.109490909091\n"
     ]
    }
   ],
   "source": [
    "mistakes = sum(ypred != ytest)\n",
    "error = mistakes/len(ytest)\n",
    "print error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of how to read a tree properties in Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The binary tree structure has 9 nodes and has the following tree structure:\n",
      "node=0 test node: go to node 1 if X[:, 350] <= 0.343137264252s else to node 2.\n",
      "\tnode=1 test node: go to node 3 if X[:, 436] <= 0.00196078442968s else to node 4.\n",
      "\tnode=2 test node: go to node 5 if X[:, 489] <= 0.0372549034655s else to node 6.\n",
      "\t\tnode=3 leaf node.\n",
      "\t\tnode=4 test node: go to node 7 if X[:, 542] <= 0.139215692878s else to node 8.\n",
      "\t\tnode=5 leaf node.\n",
      "\t\tnode=6 leaf node.\n",
      "\t\t\tnode=7 leaf node.\n",
      "\t\t\tnode=8 leaf node.\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luca/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:53: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/luca/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:49: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "estimator = DecisionTreeClassifier(max_leaf_nodes=5, random_state=0)\n",
    "estimator.fit(Xsource, ysource)\n",
    "\n",
    "# The decision estimator has an attribute called tree_  which stores the entire\n",
    "# tree structure and allows access to low level attributes. The binary tree\n",
    "# tree_ is represented as a number of parallel arrays. The i-th element of each\n",
    "# array holds information about the node `i`. Node 0 is the tree's root. NOTE:\n",
    "# Some of the arrays only apply to either leaves or split nodes, resp. In this\n",
    "# case the values of nodes of the other type are arbitrary!\n",
    "#\n",
    "# Among those arrays, we have:\n",
    "#   - left_child, id of the left child of the node\n",
    "#   - right_child, id of the right child of the node\n",
    "#   - feature, feature used for splitting the node\n",
    "#   - threshold, threshold value at the node\n",
    "#\n",
    "\n",
    "# Using those arrays, we can parse the tree structure:\n",
    "\n",
    "n_nodes = estimator.tree_.node_count\n",
    "children_left = estimator.tree_.children_left\n",
    "children_right = estimator.tree_.children_right\n",
    "feature = estimator.tree_.feature\n",
    "threshold = estimator.tree_.threshold\n",
    "\n",
    "\n",
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "node_depth = np.zeros(shape=n_nodes)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\"The binary tree structure has %s nodes and has \"\n",
    "      \"the following tree structure:\"\n",
    "      % n_nodes)\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %ss else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))\n",
    "\n",
    "#print(\"It is %s %% of all nodes.\" % (100 * len(common_node_id) / n_nodes,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
