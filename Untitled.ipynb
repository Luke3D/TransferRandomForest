{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    \n",
    "    # Corner case: If labels_in_node is empty, return 0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0\n",
    "    \n",
    "    C,unique_counts = np.unique(labels_in_node,return_counts=True) #the id of classes and number of each\n",
    "    \n",
    "    return (len(labels_in_node) - unique_counts[np.argmax(unique_counts)])\n",
    "\n",
    "\n",
    "def reached_minimum_node_size(y, min_node_size):\n",
    "    # Return True if the number of data points is less than or equal to the minimum node size.\n",
    "    if y.shape[0] <= min_node_size:\n",
    "        #print y.shape[0]\n",
    "        return True\n",
    "\n",
    "    \n",
    "# X matrix of features (p datapoints x N features)\n",
    "# y vector of labels (p x 1)\n",
    "\n",
    "def best_splitting_feature(X, y, Nbins):\n",
    "        \n",
    "    best_feature = None # Keep track of the best feature \n",
    "    best_threshold = None\n",
    "    best_I = -1     # Keep track of the best info gain so far \n",
    "\n",
    "    #the number of data points in the parent node\n",
    "    num_data_points = y.shape[0]\n",
    "    \n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in range(X.shape[1]):\n",
    "        \n",
    "        fvals = X[:,feature]\n",
    "        fvals = np.sort(fvals)  #sorting the values\n",
    "        if num_data_points > Nbins:            \n",
    "            fvals = fvals[range(0,num_data_points,Nbins)]\n",
    "        \n",
    "        #loop through all values of current feature to find the best split\n",
    "        for threshold in fvals:\n",
    "\n",
    "            # The left split will have all data points where the feature value is smaller than threshold\n",
    "            ind_left = X[:,feature] < threshold\n",
    "            left_split = X[ind_left,feature]\n",
    "             # The right split will have all data points where the feature value is larger or equal\n",
    "            ind_right = X[:,feature] >= threshold\n",
    "            right_split = X[ind_right,feature]\n",
    "            \n",
    "            #compute info-gain for current feature and threshold split\n",
    "            I = infogain(y,y[ind_left],y[ind_right])\n",
    "            \n",
    "            # If this is the best error we have found so far, store the feature as best_feature\n",
    "            # the threshold as the best threshold and the error as best_error\n",
    "            if I > best_I:\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "                best_I = I\n",
    "        \n",
    "    return best_feature, best_threshold # Return the best feature and threshold\n",
    "\n",
    "\n",
    "def infogain(yparent,yleft,yright):\n",
    "    \n",
    "    Nparent = len(yparent)\n",
    "    Nleft = len(yleft)\n",
    "    Nright = len(yright)\n",
    "    \n",
    "    #when one of the splits is empty returns I = 0\n",
    "    if Nleft ==0 or Nright == 0:\n",
    "        I = 0\n",
    "    else:\n",
    "        #compute information gain\n",
    "        I = entropy(yparent) -( (Nleft/Nparent)*entropy(yleft) + (Nright/Nparent)*entropy(yright) )   \n",
    "\n",
    "    return I\n",
    "\n",
    "\n",
    "#entropy for multiple classes\n",
    "def entropy(y):\n",
    "    C,unique_counts = np.unique(y,return_counts=True) #the id of classes and number of each\n",
    "    Pc = unique_counts/len(y)\n",
    "    H = -(Pc*np.log(Pc)).sum()\n",
    "    return H    \n",
    "\n",
    "\n",
    "def create_leaf(target_values,C):\n",
    "\n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True,\n",
    "            'prediction': None,\n",
    "            'labels_distribution':None                       }   \n",
    "    \n",
    "    # Count the number of data points of each class in the leaf.\n",
    "    C_in_node,unique_counts = np.unique(target_values,return_counts=True) #the id of classes and number of each\n",
    "    leaf['prediction'] = C_in_node[np.argmax(unique_counts)]\n",
    "    \n",
    "    Classes = np.zeros(C)\n",
    "    Classes[C_in_node] = unique_counts/len(target_values)\n",
    "    leaf['labels_distribution'] = Classes\n",
    "    \n",
    "    # Return the leaf node        \n",
    "    return leaf \n",
    "\n",
    "\n",
    "def decision_tree_create(X, y, N_features_to_sample, C, min_node_size, Nbins, Verbose, current_depth = 0, max_depth = 10):\n",
    "    \n",
    "    #randomly sample a subset of features\n",
    "    Nfeatures = X.shape[1]\n",
    "    features = np.random.choice(Nfeatures, N_features_to_sample, replace=False)    \n",
    "    \n",
    "    #select only the features sampled for this run\n",
    "    Xcurrent = X[:,features]\n",
    "    target_values = y\n",
    "\n",
    "    if Verbose == True:\n",
    "        print \"--------------------------------------------------------------------\"\n",
    "        print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "        print \"Features selected = %s\" % features\n",
    "\n",
    "\n",
    "    # Stopping condition 1\n",
    "    # (Check if there are mistakes at current node, i.e. if the node is pure.)\n",
    "    if intermediate_node_num_mistakes(target_values) == 0:  \n",
    "        if Verbose == True:\n",
    "            print \"No Mistakes at current node - Stopping.\"     \n",
    "        # If not mistakes at current node, make current node a leaf node\n",
    "        return create_leaf(target_values,C)\n",
    "    \n",
    "    #Stopping condition 2: min node size reached\n",
    "    if reached_minimum_node_size(y, min_node_size):\n",
    "        if Verbose == True:\n",
    "            print \"Minimum node size reached - Stopping\"\n",
    "        return create_leaf(y,C)\n",
    "    \n",
    "    # Stopping condition 3: (limit tree depth)\n",
    "    if current_depth >= max_depth:  \n",
    "        if Verbose == True:\n",
    "            print \"Reached maximum depth. Stopping.\"\n",
    "        # If the max tree depth has been reached, make current node a leaf node\n",
    "        return create_leaf(target_values,C)\n",
    "\n",
    "    # Find the best splitting feature and its threshold\n",
    "    splitting_feature,splitting_thres = best_splitting_feature(Xcurrent,y,Nbins)\n",
    "    splitting_feature = features[splitting_feature]\n",
    "    \n",
    "    # Split on the best feature that we found. \n",
    "    ind_left = X[:,splitting_feature] < splitting_thres\n",
    "    left_split = X[ind_left,:]\n",
    "    y_left = y[ind_left]\n",
    "\n",
    "    ind_right = X[:,splitting_feature] >= splitting_thres\n",
    "    right_split = X[ind_right,:]\n",
    "    y_right = y[ind_right]\n",
    "\n",
    "    if Verbose == True:\n",
    "        print \"Split on feature %s. (%s, %s), Threshold = %s\" % (\\\n",
    "        splitting_feature, y_left.shape, y_right.shape, splitting_thres)\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(y_left) == len(y) or len(y_right) == len(y):\n",
    "        if Verbose == True: \n",
    "            print 'One split empty: Creating Leaf'          \n",
    "        return create_leaf(y,C)  \n",
    "        \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = decision_tree_create(left_split, y_left, N_features_to_sample, C, min_node_size, Nbins, Verbose, current_depth + 1, max_depth)        \n",
    "    right_tree = decision_tree_create(right_split, y_right, N_features_to_sample, C, min_node_size, Nbins, Verbose, current_depth + 1, max_depth)\n",
    "\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'threshold'        : splitting_thres,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree,\n",
    "            'labels_distribution': None \n",
    "            \n",
    "            }\n",
    "\n",
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])\n",
    "\n",
    "def count_leaves(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1 \n",
    "    return count_leaves(tree['left']) + count_leaves(tree['right'])\n",
    "\n",
    "def classify(tree, x):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        return tree['labels_distribution'] \n",
    "    else:\n",
    "        # split on feature.\n",
    "        val_split_feature = x[tree['splitting_feature']]\n",
    "        if val_split_feature < tree['threshold']:\n",
    "            return classify(tree['left'], x)\n",
    "        else:\n",
    "            return classify(tree['right'],x)\n",
    "        \n",
    "def evaluate_classification_error_tree(tree, X, y):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    P = map(lambda x: classify(tree,x), X)\n",
    "    prediction = np.argmax(P,axis=1)\n",
    "    # Once you've made the predictions, calculate the classification error and return it\n",
    "    mistakes = sum(prediction != y)\n",
    "    error = mistakes/len(y)\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forest_create(X,y,ntrees,nvarsample=None, min_node_size = 5, Nbins = 10,max_depth=20):\n",
    "    \n",
    "    if nvarsample == None:\n",
    "        nvarsample = (np.round(np.sqrt(X.shape[1]))).astype(int)\n",
    "        print 'Nfeatures = %s'%nvarsample\n",
    "    \n",
    "    #the number of classes is inferred from the data\n",
    "    C = len(np.unique(y))\n",
    "    \n",
    "    nptrain = X.shape[0] #how many datapoints each tree is trained (same size of X)\n",
    "    RF = []\n",
    "    #for loop creating and training each tree \n",
    "    #bootstrap X to train each tree\n",
    "    for t in range(ntrees):\n",
    "        print 'current trained tree = %s'%t\n",
    "        #create bootstrap training dataset for tree t\n",
    "        indbootstrap = np.random.choice(X.shape[0],nptrain)\n",
    "        Xtree = X[indbootstrap,:]\n",
    "        ytree = y[indbootstrap] \n",
    "        \n",
    "        #train the tree\n",
    "        tree1 = decision_tree_create(Xtree,ytree,nvarsample,C,min_node_size,Nbins,Verbose=False,max_depth = max_depth)\n",
    "        RF.append(tree1)\n",
    "    \n",
    "    print 'Forest Trained!'\n",
    "    return RF\n",
    "    \n",
    "\n",
    "#outputs the posterior prob of each tree and the corresponding class\n",
    "def forest_posterior(RF,x):\n",
    "\n",
    "    T = len(RF)  #the number of trees \n",
    "\n",
    "    #infer the number of classes\n",
    "    P0 = classify(RF[0],x)\n",
    "    C = len(P0)\n",
    "    \n",
    "    Pt = np.zeros((T,C)) #matrix of posteriors from each tree (T x Nclasses)\n",
    "    Pt[0,:] = P0\n",
    "    for t in range(len(RF))[1:]:\n",
    "        Pt[t,:] = classify(RF[t],x) \n",
    "    return Pt\n",
    " \n",
    "    \n",
    "#classify input based on majority voting of each tree prediction\n",
    "def forest_classify_majority(RF,x):\n",
    "        Pt = forest_posterior(RF,x)\n",
    "        Yt = np.argmax(Pt,axis=1)         \n",
    "        C,unique_counts = np.unique(Yt,return_counts=True) #the id of classes and number of each\n",
    "        return C[np.argmax(unique_counts)]   \n",
    "    \n",
    "#classify input by averaging posteriors \n",
    "def forest_classify_ensemble(RF,x):\n",
    "    Pt = forest_posterior(RF,x)\n",
    "    Pforest = Pt.mean(axis=0)\n",
    "    ypred = np.argmax(Pt.mean(axis=0))\n",
    "    return ypred\n",
    "\n",
    "def forest_classify_geo_mean(RF,x):\n",
    "        Pt = forest_posterior(RF,x)\n",
    "        a = -np.log(Pt)\n",
    "        Pforest = a.mean(axis=0)\n",
    "        ypred = np.argmax(Pt.mean(axis=0))\n",
    "        return ypred \n",
    "    \n",
    "def evaluate_classification_error(RF, X, y, method = None):  \n",
    "    # Apply the forest_classify(RF, x) to each row in your data\n",
    "    if method == None:\n",
    "        ypred = map(lambda x: forest_classify_ensemble(RF,x), X)\n",
    "        #ypred = map(lambda x: forest_classify_majority(RF,x), X)\n",
    "        #ypred = map(lambda x: forest_classify_geo_mean(RF,x), X)\n",
    "        # Once you've made the predictions, calculate the classification error and return it\n",
    "        mistakes = sum(ypred != y)\n",
    "        error = mistakes/len(y)\n",
    "        \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a fake dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain = np.random.rand(1000,10)\n",
    "y = np.ones(1000).astype(int)\n",
    "y[(Xtrain[:,0]+Xtrain[:,1]+Xtrain[:,2] < 1)] = 0\n",
    "y[(Xtrain[:,0]+3*Xtrain[:,2] >= 1.5)] = 2\n",
    "y[(1.5*Xtrain[:,0]-2*Xtrain[:,1]+Xtrain[:,2] < 1)] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### another fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain = np.random.rand(1000,2)\n",
    "y = np.ones(1000).astype(int)\n",
    "y[(Xtrain[:,0]+Xtrain[:,1]< 1)] = 0\n",
    "y[(Xtrain[:,0]-1*Xtrain[:,1] >= 0.5)] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17299999999999999"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree1 = decision_tree_create(Xtrain, y, N_features_to_sample = 2, C = 4, min_node_size = 5, Nbins = 10, Verbose = False, current_depth = 0, max_depth = 2)\n",
    "evaluate_classification_error_tree(tree1, Xtrain, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../MNIST_data/\", one_hot=False)\n",
    "Xtest = mnist.train.images\n",
    "Xtrain = mnist.test.images\n",
    "ytest = mnist.train.labels\n",
    "ytrain = mnist.test.labels\n",
    "Xsource, Xtarget, ysource, ytarget = train_test_split(Xtrain, ytrain, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22700000000000001"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree1 = decision_tree_create(Xsource, ysource, N_features_to_sample = 28, C = 10, min_node_size = 10, Nbins = 10, Verbose = False, current_depth = 0, max_depth = 9)\n",
    "evaluate_classification_error_tree(tree1, Xtarget, ytarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_leaves(tree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_nodes(tree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current trained tree = 0\n",
      "current trained tree = 1\n",
      "current trained tree = 2\n",
      "current trained tree = 3\n",
      "current trained tree = 4\n",
      "current trained tree = 5\n",
      "current trained tree = 6\n",
      "current trained tree = 7\n",
      "Forest Trained!\n"
     ]
    }
   ],
   "source": [
    "RF = forest_create(Xsource,ysource,ntrees = 8,nvarsample=100, min_node_size = 5, Nbins = 10,max_depth=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13633333333333333"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(RF[3:8], Xtarget, ytarget, method = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26000000000000001"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error_tree(RF[1], Xtarget, ytarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
