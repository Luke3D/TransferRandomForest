{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing binary decision trees with real valued features\n",
    "**Multi class supported - Classes IDs are 0 through C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a fake dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#artificial training dataset\n",
    "Xtrain = np.random.rand(1000,2)\n",
    "y = np.ones(1000).astype(int)\n",
    "y[(Xtrain[:,0] < 0.25)] = 0\n",
    "y[(Xtrain[:,1] > 0.55)] = 2\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another fake dataset with more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 10)\n",
      "[0 1]\n",
      "[-1  1]\n"
     ]
    }
   ],
   "source": [
    "X,y = make_classification(n_samples=2000,n_features=10,n_informative=5,n_classes=2)\n",
    "print X.shape\n",
    "print np.unique(y) #original labels are 0 and 1\n",
    "ind = y==0\n",
    "y[ind] = -1\n",
    "print np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to count number of mistakes while predicting majority class\n",
    "\n",
    "Recall from the lecture that prediction at an intermediate node works by predicting the **majority class** for all data points that belong to this node.\n",
    "\n",
    "Now, we will write a function that calculates the number of **missclassified examples** when predicting the **majority class**. This will be used to help determine which feature is the best to split on at a given node of the tree.\n",
    "\n",
    "**Note**: Keep in mind that in order to compute the number of mistakes for a majority classifier, we only need the label (y values) of the data points in the node. \n",
    "\n",
    "** Steps to follow **:\n",
    "* ** Step 1:** Calculate the number of +1 and -1\n",
    "* ** Step 2:** Since we are assuming majority class prediction, all the data points that are **not** in the majority class are considered **mistakes**.\n",
    "* ** Step 3:** Return the number of **mistakes**.\n",
    "\n",
    "Now, let us write the function `intermediate_node_num_mistakes` which computes the number of misclassified examples of an intermediate node given the set of labels (y values) of the data points contained in the node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    \n",
    "    # Corner case: If labels_in_node is empty, return 0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0\n",
    "    \n",
    "    C,unique_counts = np.unique(labels_in_node,return_counts=True) #the id of classes and number of each\n",
    "    \n",
    "    return (len(labels_in_node) - unique_counts[np.argmax(unique_counts)])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reached_minimum_node_size(y, min_node_size):\n",
    "    # Return True if the number of data points is less than or equal to the minimum node size.\n",
    "    if y.shape[0] <= min_node_size:\n",
    "        print y.shape[0]\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to pick best feature to split on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **best_splitting_feature** takes 2 arguments: \n",
    "1. The feature matrix X [N datapoints x p features]\n",
    "2. The vector of labels [N x 1]\n",
    "\n",
    "Recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "* The second implementation uses the Information Gain to find the optimal split and bins the data into 10 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X matrix of features (p datapoints x N features)\n",
    "# y vector of labels (p x 1)\n",
    "\n",
    "def best_splitting_feature(X, y):\n",
    "        \n",
    "    Nbins = 10  #the number of bins to split on\n",
    "    best_feature = None # Keep track of the best feature \n",
    "    best_threshold = None\n",
    "    best_I = -1     # Keep track of the best info gain so far \n",
    "\n",
    "    #the number of data points in the parent node\n",
    "    num_data_points = y.shape[0]\n",
    "    \n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in range(X.shape[1]):\n",
    "        \n",
    "        fvals = X[:,feature]\n",
    "        fvals = np.sort(fvals)  #sorting the values\n",
    "        if num_data_points > Nbins:            \n",
    "            fvals = fvals[range(0,num_data_points,Nbins)]\n",
    "        \n",
    "        #loop through all values of current feature to find the best split\n",
    "        for threshold in fvals:\n",
    "\n",
    "            # The left split will have all data points where the feature value is smaller than threshold\n",
    "            ind_left = X[:,feature] < threshold\n",
    "            left_split = X[ind_left,feature]\n",
    "             # The right split will have all data points where the feature value is larger or equal\n",
    "            ind_right = X[:,feature] >= threshold\n",
    "            right_split = X[ind_right,feature]\n",
    "            \n",
    "            #compute info-gain for current feature and threshold split\n",
    "            I = infogain(y,y[ind_left],y[ind_right])\n",
    "            \n",
    "            # If this is the best error we have found so far, store the feature as best_feature\n",
    "            # the threshold as the best threshold and the error as best_error\n",
    "            if I > best_I:\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "                best_I = I\n",
    "        \n",
    "    return best_feature, best_threshold # Return the best feature and threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infogain(yparent,yleft,yright):\n",
    "    \n",
    "    Nparent = len(yparent)\n",
    "    Nleft = len(yleft)\n",
    "    Nright = len(yright)\n",
    "    \n",
    "    #when one of the splits is empty returns I = 0\n",
    "    if Nleft ==0 or Nright == 0:\n",
    "        I = 0\n",
    "    else:\n",
    "        #compute information gain\n",
    "        I = entropy(yparent) -( (Nleft/Nparent)*entropy(yleft) + (Nright/Nparent)*entropy(yright) )   \n",
    "\n",
    "    return I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#entropy for multiple classes\n",
    "def entropy(y):\n",
    "    C,unique_counts = np.unique(y,return_counts=True) #the id of classes and number of each\n",
    "    Pc = unique_counts/len(y)\n",
    "    H = -(Pc*np.log(Pc)).sum()\n",
    "    return H    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the tree\n",
    "\n",
    "With the above functions implemented correctly, we are now ready to build our decision tree. Each node in the decision tree is represented as a dictionary which contains the following keys and possible values:\n",
    "\n",
    "    { \n",
    "       'is_leaf'            : True/False.\n",
    "       'prediction'         : Prediction at the leaf node.\n",
    "       'left'               : (dictionary corresponding to the left tree).\n",
    "       'right'              : (dictionary corresponding to the right tree).\n",
    "       'splitting_feature'  : The feature that this node splits on.\n",
    "    }\n",
    "\n",
    "First, we will write a function that creates a leaf node given a set of target values. Fill in the places where you find `## YOUR CODE HERE`. There are **three** places in this function for you to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_leaf(target_values,C):\n",
    "\n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True,\n",
    "            'prediction': None,\n",
    "            'labels_distribution':None                       }   \n",
    "    \n",
    "    # Count the number of data points of each class in the leaf.\n",
    "    C_in_node,unique_counts = np.unique(target_values,return_counts=True) #the id of classes and number of each\n",
    "    leaf['prediction'] = C_in_node[np.argmax(unique_counts)]\n",
    "    \n",
    "    Classes = np.zeros(C)\n",
    "    Classes[C_in_node] = unique_counts/len(target_values)\n",
    "    leaf['labels_distribution'] = Classes\n",
    "    \n",
    "    # Return the leaf node        \n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decision_tree_create(X, y, N_features_to_sample, current_depth = 0, max_depth = 10, C=-1):\n",
    "\n",
    "    #parameter to set the minimum size of a node\n",
    "    min_node_size = 5\n",
    "    \n",
    "    #determine # of classes in root if the argument is not provided\n",
    "    if current_depth == 0 and C == -1:\n",
    "        C = len(np.unique(y))\n",
    "        \n",
    "    #randomly sample a subset of features\n",
    "    Nfeatures = X.shape[1]\n",
    "    features = np.random.choice(Nfeatures, N_features_to_sample, replace=False)    \n",
    "    \n",
    "    #select only the features sampled for this run\n",
    "    Xcurrent = X[:,features]\n",
    "    target_values = y\n",
    "    \n",
    "    print \"--------------------------------------------------------------------\"\n",
    "    print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "    print \"Features selected = %s\" % features\n",
    "    \n",
    "\n",
    "    # Stopping condition 1\n",
    "    # (Check if there are mistakes at current node, i.e. if the node is pure.)\n",
    "    if intermediate_node_num_mistakes(target_values) == 0:  \n",
    "        print \"No Mistakes at current node - Stopping.\"     \n",
    "        # If not mistakes at current node, make current node a leaf node\n",
    "        return create_leaf(target_values,C)\n",
    "    \n",
    "    #Stopping condition 2: min node size reached\n",
    "    if reached_minimum_node_size(y, min_node_size):\n",
    "        print \"Minimum node size reached - Stopping\"\n",
    "        return create_leaf(y,C)\n",
    "    \n",
    "    # Stopping condition 3: (limit tree depth)\n",
    "    if current_depth >= max_depth:  \n",
    "        print \"Reached maximum depth. Stopping for now.\"\n",
    "        # If the max tree depth has been reached, make current node a leaf node\n",
    "        return create_leaf(target_values,C)\n",
    "\n",
    "    # Find the best splitting feature and its threshold\n",
    "    splitting_feature,splitting_thres = best_splitting_feature(Xcurrent,y)\n",
    "    splitting_feature = features[splitting_feature]\n",
    "    \n",
    "    # Split on the best feature that we found. \n",
    "    ind_left = X[:,splitting_feature] < splitting_thres\n",
    "    left_split = X[ind_left,:]\n",
    "    y_left = y[ind_left]\n",
    "\n",
    "    ind_right = X[:,splitting_feature] >= splitting_thres\n",
    "    right_split = X[ind_right,:]\n",
    "    y_right = y[ind_right]\n",
    "            \n",
    "    #print y_left.shape, y_right.shape\n",
    "    print \"Split on feature %s. (%s, %s), Threshold = %s\" % (\\\n",
    "                      splitting_feature, y_left.shape, y_right.shape, splitting_thres)\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if y_left.shape == y.shape[0]:\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(y_left,C)\n",
    "    if y_right.shape == y.shape[0]:\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(y_right,C)\n",
    "        \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = decision_tree_create(left_split, y_left, N_features_to_sample, current_depth + 1, max_depth, C)        \n",
    "    right_tree = decision_tree_create(right_split, y_right, N_features_to_sample, current_depth + 1, max_depth, C)\n",
    "\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'threshold'        : splitting_thres,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree,\n",
    "            'labels_distribution': None \n",
    "            \n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a recursive function to count the nodes and leaves in your tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_leaves(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1 \n",
    "    return count_leaves(tree['left']) + count_leaves(tree['right'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the tree!\n",
    "\n",
    "Now that all the tests are passing, we will train a tree model on the **train_data**. Limit the depth to 6 (**max_depth = 6**) to make sure the algorithm doesn't run for too long. Call this tree **my_decision_tree**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (1000 data points).\n",
      "Features selected = [0 1]\n",
      "Split on feature 1. ((540,), (460,)), Threshold = 0.549823963967\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (540 data points).\n",
      "Features selected = [0 1]\n",
      "Split on feature 0. ((130,), (410,)), Threshold = 0.262277699549\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (130 data points).\n",
      "Features selected = [0 1]\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (410 data points).\n",
      "Features selected = [1 0]\n",
      "No Mistakes at current node - Stopping.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (460 data points).\n",
      "Features selected = [0 1]\n",
      "Split on feature 1. ((10,), (450,)), Threshold = 0.559855977152\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10 data points).\n",
      "Features selected = [1 0]\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (450 data points).\n",
      "Features selected = [0 1]\n",
      "No Mistakes at current node - Stopping.\n"
     ]
    }
   ],
   "source": [
    "tree1 = decision_tree_create(Xtrain,y,Xtrain.shape[1],max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_nodes(tree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_leaves(tree1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with a decision tree\n",
    "\n",
    "As discussed in the lecture, we can make predictions from the decision tree with a simple recursive function. Below, we call this function `classify`, which takes in a learned `tree` and a test point `x` to classify.  We include an option `annotate` that describes the prediction path when set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print \"At leaf, predicting %s\" % tree['prediction']\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # split on feature.\n",
    "        val_split_feature = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print \"Split on %s = %s\" % (tree['splitting_feature'], tree['threshold'])\n",
    "        if val_split_feature < tree['threshold']:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'],x, annotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Split on 1 = 0.549823963967\n",
      "Split on 0 = 0.262277699549\n",
      "At leaf, predicting 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print y[1]\n",
    "classify(tree1,Xtrain[1,:],annotate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': True,\n",
       " 'labels_distribution': array([ 0.1,  0. ,  0.9]),\n",
       " 'left': None,\n",
       " 'prediction': 2,\n",
       " 'right': None,\n",
       " 'splitting_feature': None}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree1['right']['left']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute the path to the leaf followed by data point x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datapath(tree, x, branch = 1):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        return branch \n",
    "    else:\n",
    "        # split on feature.\n",
    "        split_feature = tree['splitting_feature']\n",
    "        split_threshold = tree['threshold']\n",
    "\n",
    "        if x[split_feature] < split_threshold:\n",
    "            return datapath(tree['left'], x, 2*branch)\n",
    "        else:\n",
    "            return datapath(tree['right'],x, 2*branch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath(tree1,Xtrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del(tree1['right']['right']['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': True,\n",
       " 'labels_distribution': array([ 0.,  0.,  1.]),\n",
       " 'left': None,\n",
       " 'prediction': 2,\n",
       " 'right': None,\n",
       " 'splitting_feature': None}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree1['right']['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, X, y):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = map(lambda x: classify(tree,x), X)\n",
    "    # Once you've made the predictions, calculate the classification error and return it\n",
    "    mistakes = sum(prediction != y)\n",
    "    error = mistakes/len(y)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0060000000000000001"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(tree1,Xtrain,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 5, 7, 4, 5, 7, 7, 4, 7, 7, 5, 5, 7, 5, 7, 5, 5, 4, 4, 7, 5, 4, 7, 5, 7, 7, 4, 5, 7, 7, 5, 5, 7, 7, 5, 7, 4, 4, 7, 5, 5, 7, 4, 7, 5, 7, 5, 5, 5, 7, 4, 7, 5, 7, 5, 5, 5, 5, 5, 7, 7, 5, 5, 7, 4, 7, 7, 7, 7, 7, 7, 7, 5, 7, 5, 7, 7, 5, 4, 7, 7, 4, 7, 5, 5, 5, 5, 7, 5, 5, 5, 4, 5, 7, 7, 4, 5, 7, 4, 7, 4, 7, 4, 5, 5, 7, 4, 4, 7, 5, 7, 5, 7, 4, 4, 5, 7, 4, 7, 5, 5, 4, 6, 5, 5, 5, 4, 7, 5, 7, 5, 7, 7, 7, 7, 7, 7, 4, 7, 4, 7, 7, 7, 7, 5, 5, 7, 5, 5, 7, 7, 5, 4, 5, 5, 4, 7, 5, 4, 5, 7, 5, 5, 7, 7, 5, 7, 7, 7, 5, 7, 7, 7, 7, 4, 5, 7, 7, 5, 5, 5, 5, 7, 5, 5, 7, 7, 5, 5, 4, 5, 7, 7, 5, 5, 4, 7, 5, 5, 7, 7, 7, 4, 7, 5, 5, 4, 5, 7, 7, 4, 5, 4, 5, 5, 5, 4, 5, 5, 5, 7, 5, 7, 7, 5, 5, 7, 5, 6, 7, 7, 7, 4, 5, 4, 5, 7, 7, 7, 4, 7, 7, 5, 7, 5, 5, 7, 7, 7, 5, 7, 5, 7, 4, 5, 4, 5, 4, 7, 7, 7, 7, 7, 7, 7, 7, 5, 5, 5, 7, 5, 5, 7, 5, 4, 5, 5, 4, 7, 5, 5, 7, 7, 7, 7, 7, 7, 7, 5, 5, 5, 4, 7, 4, 5, 7, 5, 5, 5, 5, 7, 4, 5, 5, 5, 4, 7, 7, 5, 7, 5, 5, 7, 7, 7, 7, 7, 5, 7, 4, 5, 7, 7, 5, 7, 7, 7, 7, 7, 4, 5, 4, 7, 7, 7, 7, 5, 5, 4, 5, 5, 7, 5, 7, 7, 5, 7, 5, 5, 7, 5, 7, 5, 5, 7, 5, 5, 5, 7, 7, 7, 5, 7, 7, 5, 5, 5, 5, 7, 4, 5, 7, 7, 5, 5, 7, 4, 7, 5, 5, 5, 5, 7, 7, 7, 7, 7, 5, 4, 6, 7, 7, 7, 7, 5, 5, 7, 7, 7, 7, 5, 7, 5, 7, 4, 5, 7, 4, 5, 7, 5, 7, 5, 5, 7, 7, 5, 5, 5, 7, 7, 4, 5, 7, 5, 7, 5, 7, 7, 5, 7, 5, 7, 7, 7, 7, 4, 5, 7, 7, 5, 5, 7, 5, 7, 6, 5, 7, 7, 4, 5, 5, 7, 5, 7, 5, 4, 5, 7, 5, 7, 5, 7, 5, 5, 7, 7, 5, 4, 7, 5, 4, 5, 7, 5, 5, 7, 5, 7, 7, 5, 5, 7, 5, 4, 4, 7, 7, 5, 7, 7, 7, 7, 7, 5, 4, 5, 7, 7, 7, 4, 7, 5, 7, 5, 5, 5, 5, 5, 7, 5, 5, 4, 7, 5, 5, 7, 5, 7, 7, 4, 7, 5, 7, 7, 4, 5, 7, 5, 5, 7, 5, 5, 7, 4, 7, 4, 5, 5, 7, 4, 4, 6, 7, 7, 5, 5, 5, 7, 5, 4, 5, 7, 7, 4, 4, 7, 7, 7, 7, 7, 5, 5, 4, 7, 5, 5, 7, 4, 5, 4, 5, 5, 5, 5, 7, 4, 7, 5, 5, 7, 5, 7, 5, 4, 7, 7, 4, 7, 5, 5, 5, 4, 5, 5, 5, 7, 5, 5, 6, 7, 4, 5, 5, 4, 5, 7, 5, 7, 5, 5, 4, 5, 5, 7, 5, 4, 5, 5, 7, 7, 4, 5, 7, 5, 7, 5, 7, 7, 7, 5, 7, 5, 7, 7, 7, 7, 7, 5, 4, 5, 7, 4, 5, 5, 7, 4, 7, 7, 7, 7, 5, 7, 6, 7, 7, 7, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 4, 7, 5, 5, 5, 4, 4, 4, 5, 5, 7, 4, 4, 5, 7, 7, 4, 7, 5, 5, 5, 5, 7, 7, 7, 7, 5, 7, 7, 5, 7, 5, 5, 7, 5, 7, 7, 7, 7, 7, 5, 7, 5, 7, 7, 4, 7, 5, 5, 7, 7, 7, 7, 7, 7, 5, 7, 4, 5, 5, 7, 5, 5, 7, 7, 5, 5, 7, 7, 5, 7, 5, 4, 7, 5, 5, 4, 5, 7, 5, 7, 5, 7, 7, 5, 7, 7, 7, 4, 4, 5, 5, 5, 7, 5, 5, 7, 7, 4, 5, 5, 7, 7, 7, 6, 5, 4, 7, 7, 5, 7, 5, 7, 5, 7, 7, 5, 5, 4, 5, 5, 7, 4, 7, 5, 4, 5, 7, 5, 7, 4, 4, 7, 7, 5, 7, 7, 7, 5, 5, 5, 5, 5, 5, 7, 7, 5, 5, 7, 5, 6, 5, 5, 7, 5, 7, 5, 5, 7, 7, 5, 5, 7, 5, 5, 7, 7, 7, 7, 7, 7, 5, 5, 7, 7, 7, 7, 7, 7, 5, 7, 5, 5, 7, 5, 5, 5, 7, 7, 7, 7, 7, 5, 7, 5, 5, 7, 7, 5, 7, 4, 5, 5, 5, 5, 7, 7, 7, 5, 5, 5, 7, 4, 5, 7, 7, 7, 4, 4, 7, 7, 7, 5, 4, 7, 7, 5, 7, 5, 7, 7, 5, 7, 7, 5, 5, 5, 5, 6, 5, 5, 7, 7, 7, 5, 7, 7, 5, 7, 4, 7, 5, 7, 5, 5, 5, 4, 7, 7, 4, 7, 7, 7, 4, 5, 4, 7, 7, 5, 5, 7, 7, 4, 5, 4, 5, 5, 5, 5, 5, 7, 5, 5, 5, 4, 5, 7, 7, 5, 7, 7, 5, 7, 5, 7, 7, 5, 7, 7, 4, 5, 7, 5, 5, 7, 7, 4, 5, 5, 7, 5, 5, 5, 5, 7, 5, 5, 7, 5, 7, 7, 7, 5, 7, 5, 5, 5, 7, 7, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "leavesData1 = map(lambda x: datapath(tree1,x), Xtrain)\n",
    "print leavesData1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expansion/Reduction functions for the transfer forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Expansion/Reduction of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def expansion_reduction(tree,XT1,yT1,XT2,yT2,max_depth=2,C=2):\n",
    "\n",
    "    #Tree = tree #a copy of the tree\n",
    "    \n",
    "    #finding the leaf where each target datapoint ends up\n",
    "    leavesData1 = map(lambda x: datapath(tree,x), XT1)\n",
    "    leavesData2 = map(lambda x: datapath(tree,x), XT2)\n",
    "            \n",
    "    Uleaves1 = np.unique(leavesData1)  #the path to each leaf followed by data1\n",
    "    Uleaves2 = np.unique(leavesData2)  #the path to each leaf followed by data2\n",
    "    Uleaves = list(set(Uleaves1) & set(Uleaves2)) #leaves reached by both data1 and data2\n",
    "            \n",
    "    #expanding each leaf on the 1st bootstrap replica of target data\n",
    "    for i in Uleaves:\n",
    "        ind_data1 = leavesData1==i #indices of datapoints for each leaf\n",
    "        #set the current depth of subtree to 1 to prevent inferring the number of classes from the code\n",
    "        Exp_tree = decision_tree_create(XT1[ind_data1,:],yT1[ind_data1],XT1.shape[1],current_depth=1,max_depth=max_depth,C=C)\n",
    "\n",
    "        #Is this a good expansion?: computes classification error at each leaf for Data T2\n",
    "        ind_data2 = leavesData2==i\n",
    "        Err_leavesT2 = intermediate_node_num_mistakes(yT2[ind_data2])/len(yT2[ind_data2])\n",
    "\n",
    "        #error at the current subtree on Data T2\n",
    "        Err_subtreeT2 = evaluate_classification_error(Exp_tree, XT2[ind_data2], yT2[ind_data2])\n",
    "        \n",
    "        #comparing the error of the subtree with that at the leaf node of the original tree\n",
    "        if Err_subtreeT2 < Err_leavesT2:\n",
    "            tree = mergetrees(tree,i,Exp_tree)\n",
    "            print 'merging successful!'\n",
    "        else:\n",
    "            print 'no merging: discard subtree'\n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergetrees(tree1,leafnr,tree2):\n",
    "    leafnrbin = bin(leafnr)[3:]  #path is from the 4th element of the binary on: 0 = go left, 1 = go right\n",
    "    path = ''\n",
    "    for i in range(len(leafnrbin)):\n",
    "        if leafnrbin[i] == '0':\n",
    "            path=path+str(\"['left']\")\n",
    "        else:\n",
    "            path=path+str(\"['right']\") \n",
    "        print(path)\n",
    "    exec ('tree1'+path+\"['prediction']\"+'=None')\n",
    "    exec ('tree1'+path+\"['is_leaf']\"+'=False')\n",
    "    exec ('tree1'+path+\"['left']\"+\"=tree2['left']\")\n",
    "    exec ('tree1'+path+\"['right']\"+\"=tree2['right']\")\n",
    "    exec ('tree1'+path+\"['splitting_feature']\"+\"=tree2['splitting_feature']\")\n",
    "    exec ('tree1'+path+\"['threshold']\"+\"=tree2['threshold']\")\n",
    "    exec ('del(tree1'+path+\"['labels_distribution'])\")\n",
    "    \n",
    "#    print ('tree1'+path+\"['prediction']\"+'=None')\n",
    "#    print ('tree1'+path+\"['is_leaf']\"+'=False')\n",
    "#    print ('tree1'+path+\"['left']\"+\"=tree2['left']\")\n",
    "#    print ('tree1'+path+\"['right']\"+\"=tree2['right']\")\n",
    "#    print ('tree1'+path+\"['splitting_feature']\"+\"=tree2['splitting_feature']\")\n",
    "    \n",
    "    #print('tree1'+path+'=tree2')\n",
    "    return tree1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(350,)\n",
      "(350,)\n"
     ]
    }
   ],
   "source": [
    "Xsource, Xtarget, ysource, ytarget = train_test_split(Xtrain, y, test_size=0.7)\n",
    "XT1, XT2, yT1, yT2 = train_test_split(Xtarget, ytarget, test_size=0.5, random_state=42)\n",
    "print ysource.shape\n",
    "print yT1.shape\n",
    "print yT2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (300 data points).\n",
      "Features selected = [1 0]\n",
      "Split on feature 1. ((160,), (140,)), Threshold = 0.54470130192\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (160 data points).\n",
      "Features selected = [0 1]\n",
      "Split on feature 0. ((30,), (130,)), Threshold = 0.238050299278\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30 data points).\n",
      "Features selected = [0 1]\n",
      "No Mistakes at current node - Stopping.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (130 data points).\n",
      "Features selected = [0 1]\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (140 data points).\n",
      "Features selected = [1 0]\n",
      "Split on feature 1. ((10,), (130,)), Threshold = 0.593831430887\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10 data points).\n",
      "Features selected = [1 0]\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (130 data points).\n",
      "Features selected = [1 0]\n",
      "No Mistakes at current node - Stopping.\n"
     ]
    }
   ],
   "source": [
    "tree1 = decision_tree_create(Xsource,ysource,Xsource.shape[1],max_depth=2,C=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': False,\n",
       " 'labels_distribution': None,\n",
       " 'left': {'is_leaf': True,\n",
       "  'labels_distribution': array([ 1.,  0.,  0.]),\n",
       "  'left': None,\n",
       "  'prediction': 0,\n",
       "  'right': None,\n",
       "  'splitting_feature': None},\n",
       " 'prediction': None,\n",
       " 'right': {'is_leaf': True,\n",
       "  'labels_distribution': array([ 0.01538462,  0.98461538,  0.        ]),\n",
       "  'left': None,\n",
       "  'prediction': 1,\n",
       "  'right': None,\n",
       "  'splitting_feature': None},\n",
       " 'splitting_feature': 0,\n",
       " 'threshold': 0.23805029927820243}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree1['left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': False,\n",
       " 'labels_distribution': None,\n",
       " 'left': {'is_leaf': True,\n",
       "  'labels_distribution': array([ 0.1,  0.2,  0.7]),\n",
       "  'left': None,\n",
       "  'prediction': 2,\n",
       "  'right': None,\n",
       "  'splitting_feature': None},\n",
       " 'prediction': None,\n",
       " 'right': {'is_leaf': True,\n",
       "  'labels_distribution': array([ 0.,  0.,  1.]),\n",
       "  'left': None,\n",
       "  'prediction': 2,\n",
       "  'right': None,\n",
       "  'splitting_feature': None},\n",
       " 'splitting_feature': 1,\n",
       " 'threshold': 0.59383143088714341}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree1['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (46 data points).\n",
      "Features selected = [0 1]\n",
      "No Mistakes at current node - Stopping.\n",
      "no merging: discard subtree\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (141 data points).\n",
      "Features selected = [0 1]\n",
      "Split on feature 0. ((10,), (131,)), Threshold = 0.301046328297\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10 data points).\n",
      "Features selected = [0 1]\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (131 data points).\n",
      "Features selected = [1 0]\n",
      "No Mistakes at current node - Stopping.\n",
      "no merging: discard subtree\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (19 data points).\n",
      "Features selected = [0 1]\n",
      "Split on feature 1. ((10,), (9,)), Threshold = 0.563066280151\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10 data points).\n",
      "Features selected = [1 0]\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9 data points).\n",
      "Features selected = [1 0]\n",
      "No Mistakes at current node - Stopping.\n",
      "no merging: discard subtree\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (144 data points).\n",
      "Features selected = [1 0]\n",
      "No Mistakes at current node - Stopping.\n",
      "no merging: discard subtree\n"
     ]
    }
   ],
   "source": [
    "tree2 = expansion_reduction(tree1,XT1,yT1,XT2,yT2,max_depth=2,C=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': True,\n",
       " 'labels_distribution': array([ 0.00714286,  0.01428571,  0.97857143]),\n",
       " 'left': None,\n",
       " 'prediction': 2,\n",
       " 'right': None,\n",
       " 'splitting_feature': None}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree2['right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing out a decision stump (To be updated to the numpy version!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stump(tree, name = 'root'):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print \"(leaf, label: %s)\" % tree['prediction']\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('.')\n",
    "    print '                       %s' % name\n",
    "    print '         |---------------|----------------|'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '  [{0} == 0]               [{0} == 1]    '.format(split_name)\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '    (%s)                         (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term. 36 months == 0]               [term. 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the intermediate left subtree\n",
    "\n",
    "The tree is a recursive dictionary, so we do have access to all the nodes! We can use\n",
    "* `my_decision_tree['left']` to go left\n",
    "* `my_decision_tree['right']` to go right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       term. 36 months\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.A == 0]               [grade.A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left'], my_decision_tree['splitting_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the left subtree of the left subtree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       grade.A\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.B == 0]               [grade.B == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left']['left'], my_decision_tree['left']['splitting_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       term. 36 months\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.D == 0]               [grade.D == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (leaf, label: -1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['right'], my_decision_tree['splitting_feature'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
