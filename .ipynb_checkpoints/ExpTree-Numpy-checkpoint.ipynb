{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing binary decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to implement your own binary decision tree classifier. You will:\n",
    "    \n",
    "* Use SFrames to do some feature engineering.\n",
    "* Transform categorical variables into binary variables.\n",
    "* Write a function to compute the number of misclassified examples in an intermediate node.\n",
    "* Write a function to find the best feature to split on.\n",
    "* Build a binary decision tree from scratch.\n",
    "* Make predictions using the decision tree.\n",
    "* Evaluate the accuracy of the decision tree.\n",
    "* Visualize the decision at the root node.\n",
    "\n",
    "**Important Note**: In this assignment, we will focus on building decision trees where the data contain **only binary (0 or 1) features**. This allows us to avoid dealing with:\n",
    "* Multiple intermediate nodes in a split\n",
    "* The thresholding issues of real-valued features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire up Graphlab Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the latest version of GraphLab Create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import graphlab\n",
    "import numpy as np\n",
    "from graphlab import SArray\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load the lending club dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the same [LendingClub](https://www.lendingclub.com/) dataset as in the previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loans = graphlab.SFrame('./Data/lending-club-data.gl/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous assignment, we reassign the labels to have +1 for a safe loan, and -1 for a risky (bad) loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "loans = loans.remove_column('bad_loans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the previous assignment where we used several features, in this assignment, we will just be using 4 categorical\n",
    "features: \n",
    "\n",
    "1. grade of the loan \n",
    "2. the length of the loan term\n",
    "3. the home ownership status: own, mortgage, rent\n",
    "4. number of years of employment.\n",
    "\n",
    "Since we are building a binary decision tree, we will have to convert these categorical features to a binary representation in a subsequent section using 1-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home_ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "target = 'safe_loans'\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore what the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">grade</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">term</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">home_ownership</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">emp_length</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">safe_loans</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">B</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\"> 36 months</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">RENT</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10+ years</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">C</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\"> 60 months</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">RENT</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt; 1 year</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">C</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\"> 36 months</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">RENT</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10+ years</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">C</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\"> 36 months</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">RENT</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10+ years</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">A</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\"> 36 months</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">RENT</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3 years</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">E</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\"> 36 months</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">RENT</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9 years</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">F</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\"> 60 months</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">OWN</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4 years</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">B</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\"> 60 months</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">RENT</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">&lt; 1 year</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">C</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\"> 60 months</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">OWN</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5 years</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">B</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\"> 36 months</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">OWN</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">10+ years</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[122607 rows x 5 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tgrade\tstr\n",
       "\tterm\tstr\n",
       "\thome_ownership\tstr\n",
       "\temp_length\tstr\n",
       "\tsafe_loans\tint\n",
       "\n",
       "Rows: 122607\n",
       "\n",
       "Data:\n",
       "+-------+------------+----------------+------------+------------+\n",
       "| grade |    term    | home_ownership | emp_length | safe_loans |\n",
       "+-------+------------+----------------+------------+------------+\n",
       "|   B   |  36 months |      RENT      | 10+ years  |     1      |\n",
       "|   C   |  60 months |      RENT      |  < 1 year  |     -1     |\n",
       "|   C   |  36 months |      RENT      | 10+ years  |     1      |\n",
       "|   C   |  36 months |      RENT      | 10+ years  |     1      |\n",
       "|   A   |  36 months |      RENT      |  3 years   |     1      |\n",
       "|   E   |  36 months |      RENT      |  9 years   |     1      |\n",
       "|   F   |  60 months |      OWN       |  4 years   |     -1     |\n",
       "|   B   |  60 months |      RENT      |  < 1 year  |     -1     |\n",
       "|   C   |  60 months |      OWN       |  5 years   |     1      |\n",
       "|   B   |  36 months |      OWN       | 10+ years  |     1      |\n",
       "+-------+------------+----------------+------------+------------+\n",
       "[122607 rows x 5 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample dataset to make sure classes are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did in the previous assignment, we will undersample the larger class (safe loans) in order to balance out our dataset. This means we are throwing away many data points. We use `seed=1` so everyone gets the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of safe loans                 : 0.502236174422\n",
      "Percentage of risky loans                : 0.497763825578\n",
      "Total number of loans in our new dataset : 46508\n"
     ]
    }
   ],
   "source": [
    "safe_loans_raw = loans[loans[target] == 1]\n",
    "risky_loans_raw = loans[loans[target] == -1]\n",
    "\n",
    "# Since there are less risky loans than safe loans, find the ratio of the sizes\n",
    "# and use that percentage to undersample the safe loans.\n",
    "percentage = len(risky_loans_raw)/float(len(safe_loans_raw))\n",
    "safe_loans = safe_loans_raw.sample(percentage, seed = 1)\n",
    "risky_loans = risky_loans_raw\n",
    "loans_data = risky_loans.append(safe_loans)\n",
    "\n",
    "print \"Percentage of safe loans                 :\", len(safe_loans) / float(len(loans_data))\n",
    "print \"Percentage of risky loans                :\", len(risky_loans) / float(len(loans_data))\n",
    "print \"Total number of loans in our new dataset :\", len(loans_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are many approaches for dealing with imbalanced data, including some where we modify the learning algorithm. These approaches are beyond the scope of this course, but some of them are reviewed in this [paper](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=5128907&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F69%2F5173046%2F05128907.pdf%3Farnumber%3D5128907 ). For this assignment, we use the simplest possible approach, where we subsample the overly represented class to get a more balanced dataset. In general, and especially when the data is highly imbalanced, we recommend using more advanced methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform categorical data into binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will implement **binary decision trees** (decision trees for binary features, a specific case of categorical variables taking on two values, e.g., true/false). Since all of our features are currently categorical features, we want to turn them into binary features. \n",
    "\n",
    "For instance, the **home_ownership** feature represents the home ownership status of the loanee, which is either `own`, `mortgage` or `rent`. For example, if a data point has the feature \n",
    "```\n",
    "   {'home_ownership': 'RENT'}\n",
    "```\n",
    "we want to turn this into three features: \n",
    "```\n",
    " { \n",
    "   'home_ownership = OWN'      : 0, \n",
    "   'home_ownership = MORTGAGE' : 0, \n",
    "   'home_ownership = RENT'     : 1\n",
    " }\n",
    "```\n",
    "\n",
    "Since this code requires a few Python and GraphLab tricks, feel free to use this block of code as is. Refer to the API documentation for a deeper understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loans_data = risky_loans.append(safe_loans)\n",
    "for feature in features:\n",
    "    loans_data_one_hot_encoded = loans_data[feature].apply(lambda x: {x: 1})    \n",
    "    loans_data_unpacked = loans_data_one_hot_encoded.unpack(column_name_prefix=feature)\n",
    "    \n",
    "    # Change None's to 0's\n",
    "    for column in loans_data_unpacked.column_names():\n",
    "        loans_data_unpacked[column] = loans_data_unpacked[column].fillna(0)\n",
    "\n",
    "    loans_data.remove_column(feature)\n",
    "    loans_data.add_columns(loans_data_unpacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the feature columns look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade.A',\n",
       " 'grade.B',\n",
       " 'grade.C',\n",
       " 'grade.D',\n",
       " 'grade.E',\n",
       " 'grade.F',\n",
       " 'grade.G',\n",
       " 'term. 36 months',\n",
       " 'term. 60 months',\n",
       " 'home_ownership.MORTGAGE',\n",
       " 'home_ownership.OTHER',\n",
       " 'home_ownership.OWN',\n",
       " 'home_ownership.RENT',\n",
       " 'emp_length.1 year',\n",
       " 'emp_length.10+ years',\n",
       " 'emp_length.2 years',\n",
       " 'emp_length.3 years',\n",
       " 'emp_length.4 years',\n",
       " 'emp_length.5 years',\n",
       " 'emp_length.6 years',\n",
       " 'emp_length.7 years',\n",
       " 'emp_length.8 years',\n",
       " 'emp_length.9 years',\n",
       " 'emp_length.< 1 year',\n",
       " 'emp_length.n/a']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = loans_data.column_names()\n",
    "features.remove('safe_loans')  # Remove the response variable\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features (after binarizing categorical variables) = 25\n"
     ]
    }
   ],
   "source": [
    "print \"Number of features (after binarizing categorical variables) = %s\" % len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore what one of these columns looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype: int\n",
       "Rows: 46508\n",
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, ... ]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_data['grade.A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column is set to 1 if the loan grade is A and 0 otherwise.\n",
    "\n",
    "**Checkpoint:** Make sure the following answers match up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of grade.A loans : 6422\n",
      "Expexted answer               : 6422\n"
     ]
    }
   ],
   "source": [
    "print \"Total number of grade.A loans : %s\" % loans_data['grade.A'].sum()\n",
    "print \"Expexted answer               : 6422\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split + Split test data into target1 (for expansion) and target2 (for reduction)\n",
    "\n",
    "We split the data into a train test split with 80% of the data in the training set and 20% of the data in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = loans_data.random_split(.8, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_data1, target_data2 = test_data.random_split(.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's convert the data into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_source = train_data[features].to_numpy()\n",
    "X_target1 = target_data1[features].to_numpy()\n",
    "X_target2 = target_data2[features].to_numpy()\n",
    "\n",
    "y_source = train_data['safe_loans'].to_numpy()\n",
    "y_target1 = target_data1['safe_loans'].to_numpy()\n",
    "y_target2 = target_data2['safe_loans'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement binary decision trees from scratch. There are several steps involved in building a decision tree. For that reason, we have split the entire assignment into several sections.\n",
    "\n",
    "## Function to count number of mistakes while predicting majority class\n",
    "\n",
    "Recall from the lecture that prediction at an intermediate node works by predicting the **majority class** for all data points that belong to this node.\n",
    "\n",
    "Now, we will write a function that calculates the number of **missclassified examples** when predicting the **majority class**. This will be used to help determine which feature is the best to split on at a given node of the tree.\n",
    "\n",
    "**Note**: Keep in mind that in order to compute the number of mistakes for a majority classifier, we only need the label (y values) of the data points in the node. \n",
    "\n",
    "** Steps to follow **:\n",
    "* ** Step 1:** Calculate the number of safe loans and risky loans.\n",
    "* ** Step 2:** Since we are assuming majority class prediction, all the data points that are **not** in the majority class are considered **mistakes**.\n",
    "* ** Step 3:** Return the number of **mistakes**.\n",
    "\n",
    "\n",
    "Now, let us write the function `intermediate_node_num_mistakes` which computes the number of misclassified examples of an intermediate node given the set of labels (y values) of the data points contained in the node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    # Corner case: If labels_in_node is empty, return 0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Count the number of 1's (safe loans)\n",
    "    Safe = (labels_in_node==1).sum()\n",
    "    \n",
    "    # Count the number of -1's (risky loans)\n",
    "    Risky = (labels_in_node==-1).sum()\n",
    "                \n",
    "    # Return the number of mistakes that the majority classifier makes.\n",
    "    if Safe > Risky:\n",
    "        return Risky\n",
    "    else: \n",
    "        return Safe    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to pick best feature to split on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **best_splitting_feature** takes 3 arguments: \n",
    "1. The data (SFrame of data which includes all of the feature columns and label column)\n",
    "2. The features to consider for splits (a list of strings of column names to consider for splits)\n",
    "3. The name of the target/label column (string)\n",
    "\n",
    "The function will loop through the list of possible features, and consider splitting on each of them. It will calculate the classification error of each split and return the feature that had the smallest classification error when split on.\n",
    "\n",
    "Recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "Follow these steps: \n",
    "* **Step 1:** Loop over each feature in the feature list\n",
    "* **Step 2:** Within the loop, split the data into two groups: one group where all of the data has feature value 0 or False (we will call this the **left** split), and one group where all of the data has feature value 1 or True (we will call this the **right** split). Make sure the **left** split corresponds with 0 and the **right** split corresponds with 1 to ensure your implementation fits with our implementation of the tree building process.\n",
    "* **Step 3:** Calculate the number of misclassified examples in both groups of data and use the above formula to compute the **classification error**.\n",
    "* **Step 4:** If the computed error is smaller than the best error found so far, store this **feature and its error**.\n",
    "\n",
    "This may seem like a lot, but we have provided pseudocode in the comments in order to help you implement the function correctly.\n",
    "\n",
    "**Note:** Remember that since we are only dealing with binary features, we do not have to consider thresholds for real-valued features. This makes the implementation of this function much easier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X matrix of features (p datapoints x N features)\n",
    "# y vector of labels (p x 1)\n",
    "\n",
    "def best_splitting_feature(X, y):\n",
    "        \n",
    "    best_feature = None # Keep track of the best feature \n",
    "    best_error = 10     # Keep track of the best error so far \n",
    "    # Note: Since error is always <= 1, we should intialize it with something larger than 1.\n",
    "\n",
    "    # Convert to float to make sure error gets computed correctly.\n",
    "    num_data_points = float(len(y))  \n",
    "    \n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in range(X.shape[1]):\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        ind_left = np.where(X[:,feature] == 0)\n",
    "        left_split = X[X[:,feature] == 0]\n",
    "        \n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        ind_right = np.where(X[:,feature] == 1)\n",
    "        right_split = X[X[:,feature] == 1]\n",
    "            \n",
    "        # Calculate the number of misclassified examples in the left split.\n",
    "        # Remember that we implemented a function for this! (It was called intermediate_node_num_mistakes)\n",
    "        left_mistakes = intermediate_node_num_mistakes(y[ind_left])            \n",
    "\n",
    "        # Calculate the number of misclassified examples in the right split.\n",
    "        right_mistakes = intermediate_node_num_mistakes(y[ind_right])\n",
    "            \n",
    "        # Compute the classification error of this split.\n",
    "        # Error = (# of mistakes (left) + # of mistakes (right)) / (# of data points)\n",
    "        error = float((left_mistakes+right_mistakes)/(num_data_points))\n",
    "\n",
    "        # If this is the best error we have found so far, store the feature as best_feature and the error as best_error\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "        \n",
    "    return best_feature # Return the best feature we found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_splitting_feature(np.ones([10,5]),np.array([-1,1,1,-1,1,1,1,-1,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the tree\n",
    "\n",
    "With the above functions implemented correctly, we are now ready to build our decision tree. Each node in the decision tree is represented as a dictionary which contains the following keys and possible values:\n",
    "\n",
    "    { \n",
    "       'is_leaf'            : True/False.\n",
    "       'prediction'         : Prediction at the leaf node.\n",
    "       'left'               : (dictionary corresponding to the left tree).\n",
    "       'right'              : (dictionary corresponding to the right tree).\n",
    "       'splitting_feature'  : The feature that this node splits on.\n",
    "    }\n",
    "\n",
    "First, we will write a function that creates a leaf node given a set of target values. Fill in the places where you find `## YOUR CODE HERE`. There are **three** places in this function for you to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_leaf(target_values):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True,\n",
    "            'prediction': None   }   \n",
    "    \n",
    "    # Count the number of data points that are +1 and -1 in this node.\n",
    "    num_ones = len(target_values[target_values == +1])\n",
    "    num_minus_ones = len(target_values[target_values == -1])\n",
    "    \n",
    "    # For the leaf node, set the prediction to be the majority class.\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    if num_ones > num_minus_ones:\n",
    "        leaf['prediction'] =  1     \n",
    "    else:\n",
    "        leaf['prediction'] = -1        \n",
    "        \n",
    "    # Return the leaf node        \n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a function that learns the decision tree recursively and implements 3 stopping conditions:\n",
    "1. **Stopping condition 1:** All data points in a node are from the same class.\n",
    "2. **Stopping condition 2:** No more features to split on. (**Deprecated for now**)\n",
    "3. **Additional stopping condition:** In addition to the above two stopping conditions covered in lecture, in this assignment we will also consider a stopping condition based on the **max_depth** of the tree. By not letting the tree grow too deep, we will save computational effort in the learning process. \n",
    "\n",
    "Now, we will write down the skeleton of the learning algorithm. Fill in the places where you find `## YOUR CODE HERE`. There are **seven** places in this function for you to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decision_tree_create(X, y, N_features_to_sample, current_depth = 0, max_depth = 10):\n",
    "\n",
    "    #randomly sample a subset of features\n",
    "    Nfeatures = X.shape[1]\n",
    "    features = np.random.choice(Nfeatures, N_features_to_sample, replace=False)    \n",
    "    \n",
    "    #select only the features sampled for this run\n",
    "    Xcurrent = X[:,features]\n",
    "    target_values = y\n",
    "    \n",
    "    print \"--------------------------------------------------------------------\"\n",
    "    print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "    print \"Features selected = %s\" % features\n",
    "    \n",
    "\n",
    "    # Stopping condition 1\n",
    "    # (Check if there are mistakes at current node.\n",
    "    # Recall you wrote a function intermediate_node_num_mistakes to compute this.)\n",
    "    if intermediate_node_num_mistakes(target_values) == 0:  \n",
    "        print \"Stopping condition 1 reached.\"     \n",
    "        # If not mistakes at current node, make current node a leaf node\n",
    "        return create_leaf(target_values)\n",
    "    \n",
    "    # Stopping condition 2 (check if there are remaining features to consider splitting on)\n",
    "    # in this implementation we assume we can repeat the split on the same feature\n",
    "    #   if len(remaining_features) == 0:   \n",
    "    #       print \"Stopping condition 2 reached.\"    \n",
    "    #       # If there are no remaining features to consider, make current node a leaf node\n",
    "    #       return create_leaf(target_values)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth >= max_depth:  \n",
    "        print \"Reached maximum depth. Stopping for now.\"\n",
    "        # If the max tree depth has been reached, make current node a leaf node\n",
    "        return create_leaf(target_values)\n",
    "\n",
    "    # Find the best splitting feature (recall the function best_splitting_feature implemented above)\n",
    "    splitting_feature = best_splitting_feature(Xcurrent,y)\n",
    "    \n",
    "    # Split on the best feature that we found. \n",
    "    ind_left = np.where(Xcurrent[:,splitting_feature] == 0)\n",
    "    left_split = np.squeeze(X[ind_left,:])\n",
    "    y_left = np.squeeze(y[ind_left])\n",
    "\n",
    "    ind_right = np.where(Xcurrent[:,splitting_feature] == 1)\n",
    "    right_split = np.squeeze(X[ind_right,:])\n",
    "    y_right = np.squeeze(y[ind_right])\n",
    "    \n",
    "    #remaining_features.remove(splitting_feature) #for now we ignore the remaining features\n",
    "    print y_left.shape, y_right.shape\n",
    "    print \"Split on feature %s. (%s, %s)\" % (\\\n",
    "                      splitting_feature, len(y_left), len(y_right))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(y):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(y_left)\n",
    "    if len(right_split) == len(y):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(y_right)\n",
    "\n",
    "        \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = decision_tree_create(left_split, y_left, N_features_to_sample, current_depth + 1, max_depth)        \n",
    "    right_tree = decision_tree_create(right_split, y_right, N_features_to_sample, current_depth + 1, max_depth)\n",
    "\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 1)\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[[[1],[2]],[[3],[4]]]])\n",
    "print a.shape\n",
    "b = np.squeeze(a)\n",
    "print b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a recursive function to count the nodes in your tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_leaves(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1 \n",
    "    return count_leaves(tree['left']) + count_leaves(tree['right'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the tree!\n",
    "\n",
    "Now that all the tests are passing, we will train a tree model on the **train_data**. Limit the depth to 6 (**max_depth = 6**) to make sure the algorithm doesn't run for too long. Call this tree **my_decision_tree**. \n",
    "\n",
    "**Warning**: This code block may take 1-2 minutes to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "930600\n",
      "930600\n"
     ]
    }
   ],
   "source": [
    "print sum(sum(X_source==1))+sum(sum(X_source==0))\n",
    "print X_source.shape[0]*X_source.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luca/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (37224 data points).\n",
      "Features selected = [ 1 20 21 12 23]\n",
      "(26858,) (10366,)\n",
      "Split on feature 0. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (26858 data points).\n",
      "Features selected = [ 1  5  7 21 17]\n",
      "(8175,) (18683,)\n",
      "Split on feature 2. (8175, 18683)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8175 data points).\n",
      "Features selected = [24  7  8  9  6]\n",
      "(7983,) (192,)\n",
      "Split on feature 0. (7983, 192)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (7983 data points).\n",
      "Features selected = [14  0  7 13 11]\n",
      "(7887,) (96,)\n",
      "Split on feature 1. (7887, 96)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (7887 data points).\n",
      "Features selected = [17  0  2 20  1]\n",
      "(7388,) (499,)\n",
      "Split on feature 0. (7388, 499)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (7388 data points).\n",
      "Features selected = [21 13  1  7  4]\n",
      "(7011,) (377,)\n",
      "Split on feature 0. (7011, 377)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (7011 data points).\n",
      "Features selected = [ 2 20  8 22 12]\n",
      "(5111,) (1900,)\n",
      "Split on feature 0. (5111, 1900)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (5111 data points).\n",
      "Features selected = [20  7  5 22  0]\n",
      "(4762,) (349,)\n",
      "Split on feature 0. (4762, 349)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (4762 data points).\n",
      "Features selected = [10  2 12 24 23]\n",
      "(4759,) (3,)\n",
      "Split on feature 0. (4759, 3)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (4759 data points).\n",
      "Features selected = [ 4  5 19  0 12]\n",
      "(3039,) (1720,)\n",
      "Split on feature 0. (3039, 1720)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 10 (3039 data points).\n",
      "Features selected = [ 9 23 18 14  4]\n",
      "(1448,) (1591,)\n",
      "Split on feature 0. (1448, 1591)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 11 (1448 data points).\n",
      "Features selected = [16  6 13 11 19]\n",
      "(1292,) (156,)\n",
      "Split on feature 0. (1292, 156)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 12 (1292 data points).\n",
      "Features selected = [15  2  4 16 19]\n",
      "(1124,) (168,)\n",
      "Split on feature 0. (1124, 168)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 13 (1124 data points).\n",
      "Features selected = [ 9 23  1 18 13]\n",
      "(1124,) (0,)\n",
      "Split on feature 0. (1124, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 13 (168 data points).\n",
      "Features selected = [11  2 18 10  3]\n",
      "(149,) (19,)\n",
      "Split on feature 0. (149, 19)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 14 (149 data points).\n",
      "Features selected = [ 9 17  8 20 19]\n",
      "(149,) (0,)\n",
      "Split on feature 0. (149, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 14 (19 data points).\n",
      "Features selected = [12  3 11 24  2]\n",
      "(19,) (0,)\n",
      "Split on feature 0. (19, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 12 (156 data points).\n",
      "Features selected = [ 2  8  5  1 14]\n",
      "(156,) (0,)\n",
      "Split on feature 0. (156, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 11 (1591 data points).\n",
      "Features selected = [15 17 16 21  3]\n",
      "(1447,) (144,)\n",
      "Split on feature 0. (1447, 144)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 12 (1447 data points).\n",
      "Features selected = [ 1  8  7 14  9]\n",
      "(1447,) (0,)\n",
      "Split on feature 0. (1447, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 12 (144 data points).\n",
      "Features selected = [24 12  5 19 11]\n",
      "(144,) (0,)\n",
      "Split on feature 0. (144, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 10 (1720 data points).\n",
      "Features selected = [ 3  5 12 13 17]\n",
      "(1720,) (0,)\n",
      "Split on feature 0. (1720, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (3 data points).\n",
      "Features selected = [10 15 24 17  2]\n",
      "(0,) (3,)\n",
      "Split on feature 0. (0, 3)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (349 data points).\n",
      "Features selected = [24 15  6 10 14]\n",
      "(349,) (0,)\n",
      "Split on feature 0. (349, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (1900 data points).\n",
      "Features selected = [17 18  6  3  5]\n",
      "(1900,) (0,)\n",
      "Split on feature 0. (1900, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (377 data points).\n",
      "Features selected = [22 14  7 21 23]\n",
      "(377,) (0,)\n",
      "Split on feature 0. (377, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (499 data points).\n",
      "Features selected = [ 8 18 12  1 23]\n",
      "(0,) (499,)\n",
      "Split on feature 0. (0, 499)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (96 data points).\n",
      "Features selected = [19 14  2 18 10]\n",
      "(88,) (8,)\n",
      "Split on feature 0. (88, 8)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (88 data points).\n",
      "Features selected = [17  9 19  3 22]\n",
      "(82,) (6,)\n",
      "Split on feature 0. (82, 6)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (82 data points).\n",
      "Features selected = [11 15 18 12 17]\n",
      "(73,) (9,)\n",
      "Split on feature 0. (73, 9)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (73 data points).\n",
      "Features selected = [10  4 13 20  7]\n",
      "(73,) (0,)\n",
      "Split on feature 0. (73, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (9 data points).\n",
      "Features selected = [12 17 13 20  6]\n",
      "(9,) (0,)\n",
      "Split on feature 0. (9, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (6 data points).\n",
      "Features selected = [13  9  1 17  7]\n",
      "(6,) (0,)\n",
      "Split on feature 0. (6, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (8 data points).\n",
      "Features selected = [20 14  6  5  9]\n",
      "(8,) (0,)\n",
      "Split on feature 0. (8, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (192 data points).\n",
      "Features selected = [22 23 12 14 11]\n",
      "(192,) (0,)\n",
      "Split on feature 0. (192, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (18683 data points).\n",
      "Features selected = [21  3  2 10 13]\n",
      "(13982,) (4701,)\n",
      "Split on feature 1. (13982, 4701)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (13982 data points).\n",
      "Features selected = [19 23  5 18  3]\n",
      "(13624,) (358,)\n",
      "Split on feature 2. (13624, 358)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (13624 data points).\n",
      "Features selected = [ 4  1  6  3 24]\n",
      "(12348,) (1276,)\n",
      "Split on feature 0. (12348, 1276)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (12348 data points).\n",
      "Features selected = [20 18 14 15 16]\n",
      "(11710,) (638,)\n",
      "Split on feature 0. (11710, 638)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (11710 data points).\n",
      "Features selected = [ 6  1  3 20 14]\n",
      "(11616,) (94,)\n",
      "Split on feature 0. (11616, 94)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (11616 data points).\n",
      "Features selected = [23 16  8 20 21]\n",
      "(10466,) (1150,)\n",
      "Split on feature 0. (10466, 1150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (10466 data points).\n",
      "Features selected = [15  8 18 19  5]\n",
      "(9246,) (1220,)\n",
      "Split on feature 0. (9246, 1220)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (9246 data points).\n",
      "Features selected = [12  1 22 24  0]\n",
      "(8699,) (547,)\n",
      "Split on feature 3. (8699, 547)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 10 (8699 data points).\n",
      "Features selected = [18 19 13 17  5]\n",
      "(7764,) (935,)\n",
      "Split on feature 0. (7764, 935)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 11 (7764 data points).\n",
      "Features selected = [ 9 21 18  8  5]\n",
      "(3850,) (3914,)\n",
      "Split on feature 0. (3850, 3914)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 12 (3850 data points).\n",
      "Features selected = [ 0  5 20 12 24]\n",
      "(2538,) (1312,)\n",
      "Split on feature 0. (2538, 1312)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 13 (2538 data points).\n",
      "Features selected = [16 18 14 13 23]\n",
      "(2142,) (396,)\n",
      "Split on feature 0. (2142, 396)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 14 (2142 data points).\n",
      "Features selected = [11  0 16 19  6]\n",
      "(1817,) (325,)\n",
      "Split on feature 0. (1817, 325)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 15 (1817 data points).\n",
      "Features selected = [ 3  6 11 20  7]\n",
      "(1817,) (0,)\n",
      "Split on feature 0. (1817, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 15 (325 data points).\n",
      "Features selected = [ 6 11  2 20 23]\n",
      "(325,) (0,)\n",
      "Split on feature 0. (325, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 14 (396 data points).\n",
      "Features selected = [ 4 11 13 24  7]\n",
      "(350,) (46,)\n",
      "Split on feature 1. (350, 46)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 15 (350 data points).\n",
      "Features selected = [21 15 16  6 14]\n",
      "(350,) (0,)\n",
      "Split on feature 0. (350, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 15 (46 data points).\n",
      "Features selected = [ 4 14  5 22 19]\n",
      "(46,) (0,)\n",
      "Split on feature 0. (46, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 13 (1312 data points).\n",
      "Features selected = [ 0  8 19 24  3]\n",
      "(0,) (1312,)\n",
      "Split on feature 0. (0, 1312)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 12 (3914 data points).\n",
      "Features selected = [12  6  1 22  7]\n",
      "(3914,) (0,)\n",
      "Split on feature 0. (3914, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 11 (935 data points).\n",
      "Features selected = [24  6 14 18 13]\n",
      "(935,) (0,)\n",
      "Split on feature 0. (935, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 10 (547 data points).\n",
      "Features selected = [ 9 14  3 19  6]\n",
      "(309,) (238,)\n",
      "Split on feature 0. (309, 238)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 11 (309 data points).\n",
      "Features selected = [ 7 24 11 18 22]\n",
      "(211,) (98,)\n",
      "Split on feature 2. (211, 98)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 12 (211 data points).\n",
      "Features selected = [15 16 24 17 19]\n",
      "(211,) (0,)\n",
      "Split on feature 0. (211, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 12 (98 data points).\n",
      "Features selected = [20  2 11  3 16]\n",
      "(48,) (50,)\n",
      "Split on feature 1. (48, 50)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 13 (48 data points).\n",
      "Features selected = [16 24  4 17 20]\n",
      "(48,) (0,)\n",
      "Split on feature 0. (48, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 13 (50 data points).\n",
      "Features selected = [12 22 16 23  9]\n",
      "(50,) (0,)\n",
      "Split on feature 0. (50, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 11 (238 data points).\n",
      "Features selected = [21 12 22  3 14]\n",
      "(238,) (0,)\n",
      "Split on feature 0. (238, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (1220 data points).\n",
      "Features selected = [17  9  4 18 14]\n",
      "(1220,) (0,)\n",
      "Split on feature 0. (1220, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (1150 data points).\n",
      "Features selected = [23  2 21 13  7]\n",
      "(483,) (667,)\n",
      "Split on feature 1. (483, 667)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (483 data points).\n",
      "Features selected = [21  8  3 19  9]\n",
      "(483,) (0,)\n",
      "Split on feature 0. (483, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (667 data points).\n",
      "Features selected = [21  5  0  4  9]\n",
      "(504,) (163,)\n",
      "Split on feature 4. (504, 163)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 10 (504 data points).\n",
      "Features selected = [20 12 11  2  1]\n",
      "(504,) (0,)\n",
      "Split on feature 0. (504, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 10 (163 data points).\n",
      "Features selected = [18 13 24  8 14]\n",
      "(163,) (0,)\n",
      "Split on feature 0. (163, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (94 data points).\n",
      "Features selected = [14 10  0 11 24]\n",
      "(73,) (21,)\n",
      "Split on feature 0. (73, 21)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (73 data points).\n",
      "Features selected = [14 17 13  5 21]\n",
      "(73,) (0,)\n",
      "Split on feature 0. (73, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (21 data points).\n",
      "Features selected = [ 9  6  3 16 22]\n",
      "(9,) (12,)\n",
      "Split on feature 0. (9, 12)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (9 data points).\n",
      "Features selected = [11 10 14 21 20]\n",
      "(5,) (4,)\n",
      "Split on feature 0. (5, 4)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 10 (5 data points).\n",
      "Features selected = [ 3  9 12 20  2]\n",
      "(5,) (0,)\n",
      "Split on feature 0. (5, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 10 (4 data points).\n",
      "Features selected = [21 15  9 23 17]\n",
      "(4,) (0,)\n",
      "Split on feature 0. (4, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 9 (12 data points).\n",
      "Features selected = [11  3 12 20  0]\n",
      "(12,) (0,)\n",
      "Split on feature 0. (12, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (638 data points).\n",
      "Features selected = [17  9 23  6  2]\n",
      "(635,) (3,)\n",
      "Split on feature 3. (635, 3)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 7 (635 data points).\n",
      "Features selected = [14 15 10  2 17]\n",
      "(230,) (405,)\n",
      "Split on feature 3. (230, 405)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (230 data points).\n",
      "Features selected = [14 16  5  9  3]\n",
      "(230,) (0,)\n",
      "Split on feature 0. (230, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 8 (405 data points).\n",
      "Features selected = [10  3  0 21  7]\n",
      "(404,) ()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-ab29eacc257e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtree1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_source\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_source\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_source\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-153-ce47a6a28361>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, current_depth, max_depth)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-153-ce47a6a28361>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, current_depth, max_depth)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     return {'is_leaf'          : False, \n",
      "\u001b[0;32m<ipython-input-153-ce47a6a28361>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, current_depth, max_depth)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-153-ce47a6a28361>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, current_depth, max_depth)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-153-ce47a6a28361>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, current_depth, max_depth)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-153-ce47a6a28361>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, current_depth, max_depth)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     return {'is_leaf'          : False, \n",
      "\u001b[0;32m<ipython-input-153-ce47a6a28361>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, current_depth, max_depth)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-153-ce47a6a28361>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, current_depth, max_depth)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     return {'is_leaf'          : False, \n",
      "\u001b[0;32m<ipython-input-153-ce47a6a28361>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, current_depth, max_depth)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m#remaining_features.remove(splitting_feature) #for now we ignore the remaining features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Split on feature %s. (%s, %s)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m                      \u001b[0msplitting_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Create a leaf node if the split is \"perfect\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "tree1 = decision_tree_create(X_source,y_source,np.sqrt(X_source.shape[1]),max_depth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_nodes(tree1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with a decision tree\n",
    "\n",
    "As discussed in the lecture, we can make predictions from the decision tree with a simple recursive function. Below, we call this function `classify`, which takes in a learned `tree` and a test point `x` to classify.  We include an option `annotate` that describes the prediction path when set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print \"At leaf, predicting %s\" % tree['prediction']\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print \"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value)\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'],x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute the path to the leaf followed by data point x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datapath(tree, x, branch = 1):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        return branch \n",
    "    else:\n",
    "        # split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "\n",
    "        if split_feature_value == 0:\n",
    "            \n",
    "            return datapath(tree['left'], x, 2*branch)\n",
    "        else:\n",
    "            return datapath(tree['right'],x, 2*branch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, X, y):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = map(lambda x: classify(tree,x), X)\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error and return it\n",
    "    mistakes = sum(prediction != y)\n",
    "    error = mistakes/len(y)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53972064600611092"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(tree1,X_target1,y_target1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local expansion of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expansion_reduction(tree,dataT1,dataT2,features,target):\n",
    "\n",
    "    Tree = tree #a copy of the tree\n",
    "    \n",
    "    leavesData1 = np.zeros(len(dataT1))\n",
    "    leavesData2 = np.zeros(len(dataT2))\n",
    "    \n",
    "    for i in range(len(dataT1)):\n",
    "        leavesData1[i]=  datapath(tree,dataT1[i]) #indicates the leaf where each data point ends up\n",
    "    for i in range(len(dataT2)):\n",
    "        leavesData2[i]=  datapath(tree,dataT2[i]) #indicates the leaf where each data point ends up\n",
    "        \n",
    "    Uleaves1 = np.unique(leavesData1)  #the path to each leaf followed by data1\n",
    "    Uleaves2 = np.unique(leavesData2)  #the path to each leaf followed by data2\n",
    "    Uleaves = list(set(Uleaves1) & set(Uleaves2)) #leaves reached by both data1 and data2\n",
    "            \n",
    "    #expanding each leaf on the 1st bootstrap replica of target data\n",
    "    for i in Uleaves:\n",
    "        ind_data1 = np.where(leavesData1==i) #indices of datapoints for each leaf\n",
    "        print ind_data1\n",
    "        Exp_tree = decision_tree_create(dataT1[ind_data1], features, 'safe_loans', max_depth = 3)\n",
    "\n",
    "        #classification error at each leaf for Data T2\n",
    "        ind_data2 = np.where(leavesData2==i)\n",
    "        Err_leavesT2 = intermediate_node_num_mistakes(dataT2[ind_data2][target])/len(dataT2[ind_data2])\n",
    "\n",
    "        #error at the current subtree on Data T2\n",
    "        Err_subtreeT2 = evaluate_classification_error(Exp_tree, dataT2[ind_data2])\n",
    "        \n",
    "        #comparing the error of the subtree with that at the leaf node of the original tree\n",
    "        if Err_subtreeT2 < Err_leavesT2:\n",
    "            Tree = mergetrees(Tree,i,Exp_tree)\n",
    "    \n",
    "    return Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergetrees(tree1,leafnr,tree2):\n",
    "    leafnrbin = bin(leafnr)[3:]\n",
    "    path = ''\n",
    "    for i in range(len(leafnrbin)):\n",
    "        if leafnrbin[i] == '0':\n",
    "            path=path+str(\"['left']\")\n",
    "        else:\n",
    "            path=path+str(\"['right']\") \n",
    "        print(path)\n",
    "    exec ('tree1'+path+\"['prediction']\"+'=None')\n",
    "    exec ('tree1'+path+\"['is_leaf']\"+'=False')\n",
    "    exec ('tree1'+path+\"['left']\"+\"=tree2['left']\")\n",
    "    exec ('tree1'+path+\"['right']\"+\"=tree2['right']\")\n",
    "    exec ('tree1'+path+\"['splitting_feature']\"+\"=tree2['splitting_feature']\")\n",
    "    \n",
    "#    print ('tree1'+path+\"['prediction']\"+'=None')\n",
    "#    print ('tree1'+path+\"['is_leaf']\"+'=False')\n",
    "#    print ('tree1'+path+\"['left']\"+\"=tree2['left']\")\n",
    "#    print ('tree1'+path+\"['right']\"+\"=tree2['right']\")\n",
    "#    print ('tree1'+path+\"['splitting_feature']\"+\"=tree2['splitting_feature']\")\n",
    "    \n",
    "    #print('tree1'+path+'=tree2')\n",
    "    return tree1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1  0  0 ...,  0  0  0]\n",
      " [-1  0  1 ...,  0  0  0]\n",
      " [-1  0  0 ...,  0  0  0]\n",
      " ..., \n",
      " [ 1  0  0 ...,  0  0  0]\n",
      " [ 1  0  0 ...,  0  0  0]\n",
      " [ 1  0  0 ...,  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "t1 = target_data1.to_numpy()\n",
    "print t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid key type: must be str, bytes or type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-1d3a87e16610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexpansion_reduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_data1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_data2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-cc6880f2fff2>\u001b[0m in \u001b[0;36mexpansion_reduction\u001b[0;34m(tree, dataT1, dataT2, features, target)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mUleaves\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mind_data1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleavesData1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#indices of datapoints for each leaf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mprint\u001b[0m \u001b[0mdataT1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_data1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mExp_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataT1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_data1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'safe_loans'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luca/anaconda/lib/python2.7/site-packages/graphlab/data_structures/sframe.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3983\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3984\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0m_is_non_string_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3985\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3986\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3987\u001b[0m             \u001b[0msf_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/luca/anaconda/lib/python2.7/site-packages/graphlab/data_structures/sframe.pyc\u001b[0m in \u001b[0;36mselect_columns\u001b[0;34m(self, keylist)\u001b[0m\n\u001b[1;32m   3630\u001b[0m         if not (all([isinstance(x, str) or isinstance(x, type) or isinstance(x, bytes)\n\u001b[1;32m   3631\u001b[0m                      for x in keylist])):\n\u001b[0;32m-> 3632\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid key type: must be str, bytes or type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3634\u001b[0m         \u001b[0mcolumn_names_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid key type: must be str, bytes or type"
     ]
    }
   ],
   "source": [
    "expansion_reduction(tree1,target_data1,target_data2,features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['left']\n"
     ]
    }
   ],
   "source": [
    "tree1 = mergetrees(tree1,2,tree2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_leaves(tree2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.  5.  6.  7.]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "leaves = np.zeros(len(test_data))\n",
    "for i in range(len(test_data)):\n",
    "    leaves[i]=  datapath(my_decision_tree,test_data[i])\n",
    "    \n",
    "print np.unique(leaves)\n",
    "print count_nodes(my_decision_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': False,\n",
       " 'left': {...},\n",
       " 'prediction': None,\n",
       " 'right': {'is_leaf': True,\n",
       "  'left': None,\n",
       "  'prediction': 1,\n",
       "  'right': None,\n",
       "  'splitting_feature': None},\n",
       " 'splitting_feature': 'term. 36 months'}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_decision_tree['left']['left']['left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.,  6.,  4., ...,  7.,  6.,  6.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_leaves(my_decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_nodes(my_decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's consider the first example of the test set and see what `my_decision_tree` model predicts for this data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emp_length.1 year': 0,\n",
       " 'emp_length.10+ years': 0,\n",
       " 'emp_length.2 years': 1,\n",
       " 'emp_length.3 years': 0,\n",
       " 'emp_length.4 years': 0,\n",
       " 'emp_length.5 years': 0,\n",
       " 'emp_length.6 years': 0,\n",
       " 'emp_length.7 years': 0,\n",
       " 'emp_length.8 years': 0,\n",
       " 'emp_length.9 years': 0,\n",
       " 'emp_length.< 1 year': 0,\n",
       " 'emp_length.n/a': 0,\n",
       " 'grade.A': 0,\n",
       " 'grade.B': 0,\n",
       " 'grade.C': 0,\n",
       " 'grade.D': 1,\n",
       " 'grade.E': 0,\n",
       " 'grade.F': 0,\n",
       " 'grade.G': 0,\n",
       " 'home_ownership.MORTGAGE': 0,\n",
       " 'home_ownership.OTHER': 0,\n",
       " 'home_ownership.OWN': 0,\n",
       " 'home_ownership.RENT': 1,\n",
       " 'safe_loans': -1,\n",
       " 'term. 36 months': 0,\n",
       " 'term. 60 months': 1}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: -1 \n"
     ]
    }
   ],
   "source": [
    "print 'Predicted class: %s ' % classify(my_decision_tree, test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some annotations to our prediction to see what the prediction path was that lead to this predicted class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on term. 36 months = 0\n",
      "Split on grade.A = 0\n",
      "Split on grade.B = 0\n",
      "Split on grade.C = 0\n",
      "Split on grade.D = 1\n",
      "At leaf, predicting -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(my_decision_tree, test_data[0], annotate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Quiz question:** What was the feature that **my_decision_tree** first split on while making the prediction for test_data[0]?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Quiz question:** What was the first feature that lead to a right split of test_data[0]?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Quiz question:** What was the last feature split on before reaching a leaf node for test_data[0]?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating your decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will write a function to evaluate a decision tree by computing the classification error of the tree on the given dataset.\n",
    "\n",
    "Again, recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "Now, write a function called `evaluate_classification_error` that takes in as input:\n",
    "1. `tree` (as described above)\n",
    "2. `data` (an SFrame)\n",
    "\n",
    "This function should return a prediction (class label) for each row in `data` using the decision `tree`. Fill in the places where you find `## YOUR CODE HERE`. There is **one** place in this function for you to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "[1, -1, -1, 1, -1, 1, 1, 1, 1, 1]\n",
      "7\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "print test_data[target][0:10]\n",
    "prediction = graphlab.SArray([1,-1,-1,1,-1,1,1,1,1,1])\n",
    "print prediction\n",
    "errors = (prediction != test_data[target][0:10]).sum()\n",
    "print errors\n",
    "classerr = errors/10\n",
    "print classerr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(lambda x: classify(tree, x))\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error and return it\n",
    "    mistakes = (prediction != data[target]).sum()\n",
    "    error = mistakes/len(data[target])\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use this function to evaluate the classification error on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3837785437311504"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(my_decision_tree, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** Rounded to 2nd decimal point, what is the classification error of **my_decision_tree** on the **test_data**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error = 0.38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing out a decision stump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the lecture, we can print out a single decision stump (printing out the entire tree is left as an exercise to the curious reader). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stump(tree, name = 'root'):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print \"(leaf, label: %s)\" % tree['prediction']\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('.')\n",
    "    print '                       %s' % name\n",
    "    print '         |---------------|----------------|'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '  [{0} == 0]               [{0} == 1]    '.format(split_name)\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '    (%s)                         (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term. 36 months == 0]               [term. 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** What is the feature that is used for the split at the root node?\n",
    "\n",
    "### Exploring the intermediate left subtree\n",
    "\n",
    "The tree is a recursive dictionary, so we do have access to all the nodes! We can use\n",
    "* `my_decision_tree['left']` to go left\n",
    "* `my_decision_tree['right']` to go right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Term 36M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       term. 36 months\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.A == 0]               [grade.A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left'], my_decision_tree['splitting_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the left subtree of the left subtree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       grade.A\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.B == 0]               [grade.B == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left']['left'], my_decision_tree['left']['splitting_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz question:** What is the path of the **first 3 feature splits** considered along the **left-most** branch of **my_decision_tree**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term 36M -> grade A -> grade B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz question:** What is the path of the **first 3 feature splits** considered along the **right-most** branch of **my_decision_tree**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       term. 36 months\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.D == 0]               [grade.D == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (leaf, label: -1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['right'], my_decision_tree['splitting_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "term 36M -> grade D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
