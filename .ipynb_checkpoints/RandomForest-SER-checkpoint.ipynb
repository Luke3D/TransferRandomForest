{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing binary decision trees with real valued features\n",
    "**Multi class supported - Classes IDs are 0 through C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file u'speed_bestsplit.txt'. \n"
     ]
    }
   ],
   "source": [
    "%lprun -s -f best_splitting_feature -T speed_bestsplit.txt best_splitting_feature(Xtrain[0:100,0:28],ytrain[0:100],Nbins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    \n",
    "    # Corner case: If labels_in_node is empty, return 0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0\n",
    "    \n",
    "    C,unique_counts = np.unique(labels_in_node,return_counts=True) #the id of classes and number of each\n",
    "    \n",
    "    return (len(labels_in_node) - unique_counts[np.argmax(unique_counts)])\n",
    "\n",
    "\n",
    "def reached_minimum_node_size(y, min_node_size):\n",
    "    # Return True if the number of data points is less than or equal to the minimum node size.\n",
    "    if y.shape[0] <= min_node_size:\n",
    "        #print y.shape[0]\n",
    "        return True\n",
    "\n",
    "    \n",
    "# X matrix of features (p datapoints x N features)\n",
    "# y vector of labels (p x 1)\n",
    "\n",
    "def best_splitting_feature(X, y, Nbins):\n",
    "        \n",
    "    best_feature = None # Keep track of the best feature \n",
    "    best_threshold = None\n",
    "    best_I = -1     # Keep track of the best info gain so far \n",
    "\n",
    "    #the number of data points in the parent node\n",
    "    num_data_points = y.shape[0]\n",
    "    \n",
    "    #the entropy of the parent node\n",
    "    Hy = entropy(y)\n",
    "    \n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in range(X.shape[1]):\n",
    "        \n",
    "        fvals = X[:,feature]\n",
    "        fvals = np.sort(fvals)  #sorting the values\n",
    "        if num_data_points > Nbins:            \n",
    "            fvals = fvals[range(0,num_data_points,Nbins)]\n",
    "        \n",
    "        #loop through all values of current feature to find the best split\n",
    "        for threshold in fvals:\n",
    "\n",
    "            # The left split will have all data points where the feature value is smaller than threshold\n",
    "            ind_left = X[:,feature] < threshold\n",
    "            left_split = X[ind_left,feature]  #THIS IS USELESS - NEED TO BE REMOVED!\n",
    "             # The right split will have all data points where the feature value is larger or equal\n",
    "            ind_right = X[:,feature] >= threshold\n",
    "            right_split = X[ind_right,feature]\n",
    "            \n",
    "            #compute info-gain for current feature and threshold split\n",
    "            I = infogain(Hy,len(y),y[ind_left],y[ind_right])\n",
    "            \n",
    "            # If this is the best error we have found so far, store the feature as best_feature\n",
    "            # the threshold as the best threshold and the error as best_error\n",
    "            if I > best_I:\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "                best_I = I\n",
    "        \n",
    "    return best_feature, best_threshold # Return the best feature and threshold\n",
    "\n",
    "\n",
    "def infogain(Hy,Nparent,yleft,yright):\n",
    "    \n",
    "    Nleft = len(yleft)\n",
    "    Nright = len(yright)\n",
    "    \n",
    "    #when one of the splits is empty returns I = 0\n",
    "    if Nleft ==0 or Nright == 0:\n",
    "        I = 0\n",
    "    else:\n",
    "        #compute information gain\n",
    "        I = Hy -( (Nleft/Nparent)*entropy(yleft) + (Nright/Nparent)*entropy(yright) )   \n",
    "\n",
    "    return I\n",
    "\n",
    "\n",
    "#entropy for multiple classes\n",
    "def entropy(y):\n",
    "    C,unique_counts = np.unique(y,return_counts=True) #the id of classes and number of each\n",
    "    Pc = unique_counts/len(y)\n",
    "    H = -(Pc*np.log(Pc)).sum()\n",
    "    return H    \n",
    "\n",
    "\n",
    "def create_leaf(target_values,C):\n",
    "\n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True,\n",
    "            'prediction': None,\n",
    "            'labels_distribution':None                       }   \n",
    "    \n",
    "    # Count the number of data points of each class in the leaf.\n",
    "    C_in_node,unique_counts = np.unique(target_values,return_counts=True) #the id of classes and number of each\n",
    "    leaf['prediction'] = C_in_node[np.argmax(unique_counts)]\n",
    "    \n",
    "    Classes = np.zeros(C)\n",
    "    Classes[C_in_node] = unique_counts/len(target_values)\n",
    "    leaf['labels_distribution'] = Classes\n",
    "    \n",
    "    # Return the leaf node        \n",
    "    return leaf \n",
    "\n",
    "\n",
    "def decision_tree_create(X, y, N_features_to_sample, C, min_node_size, Nbins, Verbose, current_depth = 0, max_depth = 10):\n",
    "    \n",
    "    #randomly sample a subset of features\n",
    "    Nfeatures = X.shape[1]\n",
    "    features = np.random.choice(Nfeatures, N_features_to_sample, replace=False)    \n",
    "    \n",
    "    #select only the features sampled for this run\n",
    "    Xcurrent = X[:,features]\n",
    "    target_values = y\n",
    "\n",
    "    if Verbose == True:\n",
    "        print \"--------------------------------------------------------------------\"\n",
    "        print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "        print \"Features selected = %s\" % features\n",
    "\n",
    "\n",
    "    # Stopping condition 1\n",
    "    # (Check if there are mistakes at current node, i.e. if the node is pure.)\n",
    "    if intermediate_node_num_mistakes(target_values) == 0:  \n",
    "        if Verbose == True:\n",
    "            print \"No Mistakes at current node - Stopping.\"     \n",
    "        # If not mistakes at current node, make current node a leaf node\n",
    "        return create_leaf(target_values,C)\n",
    "    \n",
    "    #Stopping condition 2: min node size reached\n",
    "    if reached_minimum_node_size(y, min_node_size):\n",
    "        if Verbose == True:\n",
    "            print \"Minimum node size reached - Stopping\"\n",
    "        return create_leaf(y,C)\n",
    "    \n",
    "    # Stopping condition 3: (limit tree depth)\n",
    "    if current_depth >= max_depth:  \n",
    "        if Verbose == True:\n",
    "            print \"Reached maximum depth. Stopping.\"\n",
    "        # If the max tree depth has been reached, make current node a leaf node\n",
    "        return create_leaf(target_values,C)\n",
    "\n",
    "    # Find the best splitting feature and its threshold\n",
    "    splitting_feature,splitting_thres = best_splitting_feature(Xcurrent,y,Nbins)\n",
    "    splitting_feature = features[splitting_feature]\n",
    "    \n",
    "    # Split on the best feature that we found. \n",
    "    ind_left = X[:,splitting_feature] < splitting_thres\n",
    "    left_split = X[ind_left,:]\n",
    "    y_left = y[ind_left]\n",
    "\n",
    "    ind_right = X[:,splitting_feature] >= splitting_thres\n",
    "    right_split = X[ind_right,:]\n",
    "    y_right = y[ind_right]\n",
    "\n",
    "    if Verbose == True:\n",
    "        print \"Split on feature %s. (%s, %s), Threshold = %s\" % (\\\n",
    "        splitting_feature, y_left.shape, y_right.shape, splitting_thres)\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(y_left) == len(y) or len(y_right) == len(y):\n",
    "        if Verbose == True: \n",
    "            print 'One split empty: Creating Leaf'          \n",
    "        return create_leaf(y,C)  \n",
    "        \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = decision_tree_create(left_split, y_left, N_features_to_sample, C, min_node_size, Nbins, Verbose, current_depth + 1, max_depth)        \n",
    "    right_tree = decision_tree_create(right_split, y_right, N_features_to_sample, C, min_node_size, Nbins, Verbose, current_depth + 1, max_depth)\n",
    "\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'threshold'        : splitting_thres,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree,\n",
    "            'labels_distribution': None \n",
    "            \n",
    "            }\n",
    "\n",
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])\n",
    "\n",
    "def count_leaves(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1 \n",
    "    return count_leaves(tree['left']) + count_leaves(tree['right'])\n",
    "\n",
    "def classify(tree, x):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        return tree['labels_distribution'] \n",
    "    else:\n",
    "        # split on feature.\n",
    "        val_split_feature = x[tree['splitting_feature']]\n",
    "        if val_split_feature < tree['threshold']:\n",
    "            return classify(tree['left'], x)\n",
    "        else:\n",
    "            return classify(tree['right'],x)\n",
    "        \n",
    "def evaluate_classification_error_tree(tree, X, y):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    P = map(lambda x: classify(tree,x), X)\n",
    "    prediction = np.argmax(P,axis=1)\n",
    "    # Once you've made the predictions, calculate the classification error and return it\n",
    "    mistakes = sum(prediction != y)\n",
    "    error = mistakes/len(y)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forest functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forest_create(X,y,ntrees,nvarsample=None, min_node_size = 5, Nbins = 10,max_depth=20):\n",
    "    \n",
    "    if nvarsample == None:\n",
    "        nvarsample = (np.round(np.sqrt(X.shape[1]))).astype(int)\n",
    "        print 'Nfeatures = %s'%nvarsample\n",
    "    \n",
    "    #the number of classes is inferred from the data\n",
    "    C = len(np.unique(y))\n",
    "    \n",
    "    nptrain = X.shape[0] #how many datapoints each tree is trained (same size of X)\n",
    "    RF = []\n",
    "    #for loop creating and training each tree \n",
    "    #bootstrap X to train each tree\n",
    "    for t in range(ntrees):\n",
    "        print 'current trained tree = %s'%t\n",
    "        #create bootstrap training dataset for tree t\n",
    "        indbootstrap = np.random.choice(X.shape[0],nptrain)\n",
    "        Xtree = X[indbootstrap,:]\n",
    "        ytree = y[indbootstrap] \n",
    "        \n",
    "        #train the tree\n",
    "        tree1 = decision_tree_create(Xtree,ytree,nvarsample,C,min_node_size,Nbins,Verbose=False,max_depth = max_depth)\n",
    "        RF.append(tree1)\n",
    "    \n",
    "    print 'Forest Trained!'\n",
    "    return RF\n",
    "    \n",
    "\n",
    "#outputs the posterior prob of each tree and the corresponding class\n",
    "def forest_posterior(RF,x):\n",
    "\n",
    "    T = len(RF)  #the number of trees \n",
    "\n",
    "    #infer the number of classes\n",
    "    P0 = classify(RF[0],x)\n",
    "    C = len(P0)\n",
    "    \n",
    "    Pt = np.zeros((T,C)) #matrix of posteriors from each tree (T x Nclasses)\n",
    "    Pt[0,:] = P0\n",
    "    for t in range(len(RF))[1:]:\n",
    "        Pt[t,:] = classify(RF[t],x) \n",
    "    return Pt\n",
    " \n",
    "    \n",
    "#classify input based on majority voting of each tree prediction\n",
    "def forest_classify_majority(RF,x):\n",
    "        Pt = forest_posterior(RF,x)\n",
    "        Yt = np.argmax(Pt,axis=1)         \n",
    "        C,unique_counts = np.unique(Yt,return_counts=True) #the id of classes and number of each\n",
    "        return C[np.argmax(unique_counts)]   \n",
    "    \n",
    "#classify input by averaging posteriors \n",
    "def forest_classify_ensemble(RF,x):\n",
    "    Pt = forest_posterior(RF,x)\n",
    "    Pforest = Pt.mean(axis=0)\n",
    "    ypred = np.argmax(Pt.mean(axis=0))\n",
    "    return ypred\n",
    "\n",
    "def evaluate_classification_error(RF, X, y, method = None):  \n",
    "    # Apply the forest_classify(RF, x) to each row in your data\n",
    "    if method == None:\n",
    "        ypred = map(lambda x: forest_classify_ensemble(RF,x), X)\n",
    "        # Once you've made the predictions, calculate the classification error and return it\n",
    "        mistakes = sum(ypred != y)\n",
    "        error = mistakes/len(y)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SER Algorithm functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Refine the trained forest on target data using the SER algorithm\n",
    "def forest_SER(RF,XT,yT,C,max_depth=2):\n",
    "\n",
    "    nptrain = len(yT) #how many datapoints each tree is trained (same size of yT)\n",
    "    ntrees = len(RF)\n",
    "    RFnew = []\n",
    "\n",
    "    for t in range(ntrees):\n",
    "        print 'expanding/reducing tree = %s'%t\n",
    "        #Bootstrap XT1 and XT2\n",
    "        indbootstrap1 = np.random.choice(XT.shape[0],nptrain)\n",
    "        indbootstrap2 = np.random.choice(XT.shape[0],nptrain)\n",
    "        XT1 = XT[indbootstrap1,:]\n",
    "        XT2 = XT[indbootstrap2,:]\n",
    "        yT1 = yT[indbootstrap1]\n",
    "        yT2 = yT[indbootstrap2]\n",
    "\n",
    "        treeNew = expansion_reduction(RF[t],XT1,yT1,XT2,yT2,max_depth,C)\n",
    "        RFnew.append(treeNew)\n",
    "        \n",
    "    print 'Forest refined on target data!'\n",
    "    return RFnew\n",
    "\n",
    "\n",
    "# compute the path to the leaf followed by data point x  \n",
    "def datapath(tree, x, branch = 1):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        return branch \n",
    "    else:\n",
    "        # split on feature.\n",
    "        split_feature = tree['splitting_feature']\n",
    "        split_threshold = tree['threshold']\n",
    "\n",
    "        if x[split_feature] < split_threshold:\n",
    "            return datapath(tree['left'], x, 2*branch)\n",
    "        else:\n",
    "            return datapath(tree['right'],x, 2*branch+1)\n",
    "\n",
    "        \n",
    "def expansion_reduction(tree,XT1,yT1,XT2,yT2,max_depth=2,C=-1,min_node_size = 5):\n",
    "\n",
    "    #Tree = tree #a copy of the tree\n",
    "    \n",
    "    #finding the leaf where each target datapoint ends up\n",
    "    leavesData1 = map(lambda x: datapath(tree,x), XT1)\n",
    "    leavesData2 = map(lambda x: datapath(tree,x), XT2)\n",
    "            \n",
    "    Uleaves1 = np.unique(leavesData1)  #the path to each leaf followed by data1\n",
    "    Uleaves2 = np.unique(leavesData2)  #the path to each leaf followed by data2\n",
    "    Uleaves = list(set(Uleaves1) & set(Uleaves2)) #leaves reached by both data1 and data2\n",
    "            \n",
    "    #expanding each leaf on the 1st bootstrap replica of target data\n",
    "    for i in Uleaves:\n",
    "        ind_data1 = leavesData1==i #indices of datapoints for each leaf\n",
    "        #set the current depth of subtree to 1 to prevent inferring the number of classes from the code\n",
    "        Exp_tree = decision_tree_create(XT1[ind_data1,:],yT1[ind_data1],XT1.shape[1], C, min_node_size, Nbins = 10, Verbose = False, current_depth=1,max_depth=max_depth)\n",
    "\n",
    "        #Is this a good expansion?: computes classification error at each leaf for Data T2\n",
    "        ind_data2 = leavesData2==i\n",
    "        Err_leavesT2 = intermediate_node_num_mistakes(yT2[ind_data2])/len(yT2[ind_data2])\n",
    "\n",
    "        #error at the current subtree on Data T2\n",
    "        Err_subtreeT2 = evaluate_classification_error_tree(Exp_tree, XT2[ind_data2], yT2[ind_data2])\n",
    "        \n",
    "        #comparing the error of the subtree with that at the leaf node of the original tree\n",
    "        if Err_subtreeT2 < Err_leavesT2:\n",
    "            tree = mergetrees(tree,i,Exp_tree)\n",
    "            print 'merging successful!'\n",
    "        else:\n",
    "            print 'no merging: discard subtree'\n",
    "    \n",
    "    return tree\n",
    "\n",
    "\n",
    "def mergetrees(tree1,leafnr,tree2):\n",
    "    leafnrbin = bin(leafnr)[3:]  #path is from the 4th element of the binary on: 0 = go left, 1 = go right\n",
    "    path = ''\n",
    "    for i in range(len(leafnrbin)):\n",
    "        if leafnrbin[i] == '0':\n",
    "            path=path+str(\"['left']\")\n",
    "        else:\n",
    "            path=path+str(\"['right']\") \n",
    "    # print(path)\n",
    "    exec ('tree1'+path+\"['prediction']\"+'=None')\n",
    "    exec ('tree1'+path+\"['is_leaf']\"+'=False')\n",
    "    exec ('tree1'+path+\"['left']\"+\"=tree2['left']\")\n",
    "    exec ('tree1'+path+\"['right']\"+\"=tree2['right']\")\n",
    "    exec ('tree1'+path+\"['splitting_feature']\"+\"=tree2['splitting_feature']\")\n",
    "    exec ('tree1'+path+\"['threshold']\"+\"=tree2['threshold']\")\n",
    "    exec ('del(tree1'+path+\"['labels_distribution'])\")\n",
    "    \n",
    "#    print ('tree1'+path+\"['prediction']\"+'=None')\n",
    "#    print ('tree1'+path+\"['is_leaf']\"+'=False')\n",
    "#    print ('tree1'+path+\"['left']\"+\"=tree2['left']\")\n",
    "#    print ('tree1'+path+\"['right']\"+\"=tree2['right']\")\n",
    "#    print ('tree1'+path+\"['splitting_feature']\"+\"=tree2['splitting_feature']\")\n",
    "    \n",
    "    #print('tree1'+path+'=tree2')\n",
    "    return tree1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a fake dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#artificial training dataset\n",
    "Xtrain = np.random.rand(1000,10)\n",
    "y = np.ones(1000).astype(int)\n",
    "y[(Xtrain[:,0] < 0.25)] = 0\n",
    "y[(Xtrain[:,1] > 0.55)] = 2\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A linearly separable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain = np.random.rand(1000,10)\n",
    "y = np.ones(1000).astype(int)\n",
    "y[(Xtrain[:,0]+Xtrain[:,1]+Xtrain[:,2] < 1)] = 0\n",
    "y[(Xtrain[:,0]+3*Xtrain[:,2] >= 1.5)] = 2\n",
    "y[(1.5*Xtrain[:,0]-2*Xtrain[:,1]+Xtrain[:,2] < 1)] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another fake dataset with more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 10)\n",
      "[0 1]\n",
      "[-1  1]\n"
     ]
    }
   ],
   "source": [
    "X,y = make_classification(n_samples=2000,n_features=10,n_informative=5,n_classes=2)\n",
    "print X.shape\n",
    "print np.unique(y) #original labels are 0 and 1\n",
    "ind = y==0\n",
    "y[ind] = -1\n",
    "print np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../MNIST_data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1029def10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD8CAYAAABTq8lnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVAgTvGFUCBO8YVQIE7xhVAgTvGFUiP8PuUrQomxhJCIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b9c2cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot one MNIST datapoint\n",
    "xtest = mnist.test.images[22,:]\n",
    "xtest = xtest.reshape(28,28)\n",
    "plt.imshow(xtest,'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using test as train and train as test to make it faster (need to change later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(55000, 784)\n",
      "(10000,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "Xtest = mnist.train.images\n",
    "Xtrain = mnist.test.images\n",
    "ytest = mnist.train.labels\n",
    "ytrain = mnist.test.labels\n",
    "print Xtrain.shape\n",
    "print Xtest.shape\n",
    "print ytrain.shape\n",
    "print np.unique(ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split + Split test data into target1 (for expansion) and target2 (for reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000,)\n",
      "(3000,)\n"
     ]
    }
   ],
   "source": [
    "Xsource, Xtarget, ysource, ytarget = train_test_split(Xtrain, ytrain, test_size=0.3)\n",
    "print ysource.shape\n",
    "print ytarget.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can train on our version of random forest to compare the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nfeatures = 28\n",
      "current trained tree = 0\n",
      "current trained tree = 1\n",
      "current trained tree = 2\n",
      "Forest Trained!\n"
     ]
    }
   ],
   "source": [
    "RF_MNIST = forest_create(Xsource,ysource,3,max_depth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21909090909090909"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(RF_MNIST,Xtest,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand the forest on the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_leaves(RF_MNIST[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3186181818181818"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error_tree(RF_MNIST[0],Xtest,ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expanding/reducing tree = 0\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "expanding/reducing tree = 1\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "expanding/reducing tree = 2\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "no merging: discard subtree\n",
      "merging successful!\n",
      "merging successful!\n",
      "no merging: discard subtree\n",
      "Forest refined on target data!\n"
     ]
    }
   ],
   "source": [
    "new_RF = forest_SER(RF_MNIST,Xtarget,ytarget,C=10,max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "423"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_leaves(new_RF[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23961818181818181"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(new_RF,Xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test the Random forest classifier from Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(Xsource, ysource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.104509090909\n"
     ]
    }
   ],
   "source": [
    "mistakes = sum(ypred != ytest)\n",
    "error = mistakes/len(ytest)\n",
    "print error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
