{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing binary decision trees with real valued features\n",
    "**Multi class supported - Classes IDs are 0 through C**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a fake dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#artificial training dataset\n",
    "Xtrain = np.random.rand(1000,10)\n",
    "y = np.ones(1000).astype(int)\n",
    "y[(Xtrain[:,0] < 0.25)] = 0\n",
    "y[(Xtrain[:,1] > 0.55)] = 2\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A linearly separable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain = np.random.rand(1000,10)\n",
    "y = np.ones(1000).astype(int)\n",
    "y[(Xtrain[:,0]+Xtrain[:,1]+Xtrain[:,2] < 1)] = 0\n",
    "y[(Xtrain[:,0]+3*Xtrain[:,2] >= 1.5)] = 2\n",
    "y[(1.5*Xtrain[:,0]-2*Xtrain[:,1]+Xtrain[:,2] < 1)] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another fake dataset with more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 10)\n",
      "[0 1]\n",
      "[-1  1]\n"
     ]
    }
   ],
   "source": [
    "X,y = make_classification(n_samples=2000,n_features=10,n_informative=5,n_classes=2)\n",
    "print X.shape\n",
    "print np.unique(y) #original labels are 0 and 1\n",
    "ind = y==0\n",
    "y[ind] = -1\n",
    "print np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split + Split test data into target1 (for expansion) and target2 (for reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340, 10)\n"
     ]
    }
   ],
   "source": [
    "X_source, X_test, y_source, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print X_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(495, 10)\n"
     ]
    }
   ],
   "source": [
    "X_target1, X_target2, y_target1, y_target2 = train_test_split(X_test,y_test)\n",
    "print X_target1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to count number of mistakes while predicting majority class\n",
    "\n",
    "Recall from the lecture that prediction at an intermediate node works by predicting the **majority class** for all data points that belong to this node.\n",
    "\n",
    "Now, we will write a function that calculates the number of **missclassified examples** when predicting the **majority class**. This will be used to help determine which feature is the best to split on at a given node of the tree.\n",
    "\n",
    "**Note**: Keep in mind that in order to compute the number of mistakes for a majority classifier, we only need the label (y values) of the data points in the node. \n",
    "\n",
    "** Steps to follow **:\n",
    "* ** Step 1:** Calculate the number of +1 and -1\n",
    "* ** Step 2:** Since we are assuming majority class prediction, all the data points that are **not** in the majority class are considered **mistakes**.\n",
    "* ** Step 3:** Return the number of **mistakes**.\n",
    "\n",
    "Now, let us write the function `intermediate_node_num_mistakes` which computes the number of misclassified examples of an intermediate node given the set of labels (y values) of the data points contained in the node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    \n",
    "    # Corner case: If labels_in_node is empty, return 0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0\n",
    "    \n",
    "    C,unique_counts = np.unique(labels_in_node,return_counts=True) #the id of classes and number of each\n",
    "    \n",
    "    return (len(labels_in_node) - unique_counts[np.argmax(unique_counts)])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reached_minimum_node_size(y, min_node_size):\n",
    "    # Return True if the number of data points is less than or equal to the minimum node size.\n",
    "    if y.shape[0] <= min_node_size:\n",
    "        #print y.shape[0]\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to pick best feature to split on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **best_splitting_feature** takes 2 arguments: \n",
    "1. The feature matrix X [N datapoints x p features]\n",
    "2. The vector of labels [N x 1]\n",
    "\n",
    "Recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "* The second implementation uses the Information Gain to find the optimal split and bins the data into 10 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X matrix of features (p datapoints x N features)\n",
    "# y vector of labels (p x 1)\n",
    "\n",
    "def best_splitting_feature(X, y, Nbins):\n",
    "        \n",
    "    best_feature = None # Keep track of the best feature \n",
    "    best_threshold = None\n",
    "    best_I = -1     # Keep track of the best info gain so far \n",
    "\n",
    "    #the number of data points in the parent node\n",
    "    num_data_points = y.shape[0]\n",
    "    \n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in range(X.shape[1]):\n",
    "        \n",
    "        fvals = X[:,feature]\n",
    "        fvals = np.sort(fvals)  #sorting the values\n",
    "        if num_data_points > Nbins:            \n",
    "            fvals = fvals[range(0,num_data_points,Nbins)]\n",
    "        #else:\n",
    "        #    fvals = fvals[1:-1] #exclude the first and last data point to prevent empty splits\n",
    "\n",
    "        \n",
    "        #loop through all values of current feature to find the best split\n",
    "        for threshold in fvals:\n",
    "\n",
    "            # The left split will have all data points where the feature value is smaller than threshold\n",
    "            ind_left = np.where(X[:,feature] < threshold)\n",
    "            left_split = np.squeeze(X[X[:,feature] < threshold])\n",
    "            if np.array(ind_left).shape[1]==0:\n",
    "                #print 'left split empty'\n",
    "                continue\n",
    "\n",
    "            # The right split will have all data points where the feature value is larger or equal\n",
    "            ind_right = np.where(X[:,feature] >= threshold)\n",
    "            right_split = np.squeeze(X[X[:,feature] >= threshold])\n",
    "            if np.array(ind_right).shape[1]==0:\n",
    "                #print 'right split empty'\n",
    "                continue\n",
    "            \n",
    "            #compute info-gain for current feature and threshold split\n",
    "            I = infogain(y,y[ind_left],y[ind_right])\n",
    "            \n",
    "            # If this is the best error we have found so far, store the feature as best_feature\n",
    "            # the threshold as the best threshold and the error as best_error\n",
    "            if I > best_I:\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "                best_I = I\n",
    "        \n",
    "    return best_feature, best_threshold # Return the best feature and threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infogain(yparent,yleft,yright):\n",
    "    \n",
    "    Nparent = len(yparent)\n",
    "    Nleft = len(yleft)\n",
    "    Nright = len(yright)\n",
    "    \n",
    "    if Nleft ==0 or Nright == 0:\n",
    "        I = 0\n",
    "    else:\n",
    "        #information gain\n",
    "        I = entropy(yparent) -( (Nleft/Nparent)*entropy(yleft) + (Nright/Nparent)*entropy(yright) )   \n",
    "\n",
    "    return I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#entropy for multiple classes\n",
    "def entropy(y):\n",
    "    C,unique_counts = np.unique(y,return_counts=True) #the id of classes and number of each\n",
    "    Pc = unique_counts/len(y)\n",
    "    H = -(Pc*np.log(Pc)).sum()\n",
    "    return H    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the tree\n",
    "\n",
    "With the above functions implemented correctly, we are now ready to build our decision tree. Each node in the decision tree is represented as a dictionary which contains the following keys and possible values:\n",
    "\n",
    "    { \n",
    "       'is_leaf'            : True/False.\n",
    "       'prediction'         : Prediction at the leaf node.\n",
    "       'left'               : (dictionary corresponding to the left tree).\n",
    "       'right'              : (dictionary corresponding to the right tree).\n",
    "       'splitting_feature'  : The feature that this node splits on.\n",
    "    }\n",
    "\n",
    "First, we will write a function that creates a leaf node given a set of target values. Fill in the places where you find `## YOUR CODE HERE`. There are **three** places in this function for you to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_leaf(target_values,C):\n",
    "\n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True,\n",
    "            'prediction': None,\n",
    "            'labels_distribution':None                       }   \n",
    "    \n",
    "    # Count the number of data points of each class in the leaf.\n",
    "    C_in_node,unique_counts = np.unique(target_values,return_counts=True) #the id of classes and number of each\n",
    "    leaf['prediction'] = C_in_node[np.argmax(unique_counts)]\n",
    "    \n",
    "    Classes = np.zeros(C)\n",
    "    Classes[C_in_node] = unique_counts/len(target_values)\n",
    "    leaf['labels_distribution'] = Classes\n",
    "    \n",
    "    # Return the leaf node        \n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decision_tree_create(X, y, N_features_to_sample, C, min_node_size, Nbins, current_depth = 0, max_depth = 10, Verbose = False):\n",
    "    \n",
    "    #randomly sample a subset of features\n",
    "    Nfeatures = X.shape[1]\n",
    "    features = np.random.choice(Nfeatures, N_features_to_sample, replace=False)    \n",
    "    \n",
    "    #select only the features sampled for this run\n",
    "    Xcurrent = X[:,features]\n",
    "    target_values = y\n",
    "\n",
    "    if Verbose == True:\n",
    "        print \"--------------------------------------------------------------------\"\n",
    "        print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "        print \"Features selected = %s\" % features\n",
    "\n",
    "\n",
    "    # Stopping condition 1\n",
    "    # (Check if there are mistakes at current node, i.e. if the node is pure.)\n",
    "    if intermediate_node_num_mistakes(target_values) == 0:  \n",
    "        #print \"No Mistakes at current node - Stopping.\"     \n",
    "        # If not mistakes at current node, make current node a leaf node\n",
    "        return create_leaf(target_values,C)\n",
    "    \n",
    "    #Stopping condition 2: min node size reached\n",
    "    if reached_minimum_node_size(y, min_node_size):\n",
    "        #print \"Minimum node size reached - Stopping\"\n",
    "        return create_leaf(y,C)\n",
    "    \n",
    "    # Stopping condition 3: (limit tree depth)\n",
    "    if current_depth >= max_depth:  \n",
    "        #print \"Reached maximum depth. Stopping for now.\"\n",
    "        # If the max tree depth has been reached, make current node a leaf node\n",
    "        return create_leaf(target_values,C)\n",
    "\n",
    "    # Find the best splitting feature and its threshold\n",
    "    splitting_feature,splitting_thres = best_splitting_feature(Xcurrent,y, Nbins)\n",
    "    splitting_feature = features[splitting_feature]\n",
    "    \n",
    "    # Split on the best feature that we found. \n",
    "    try:\n",
    "        ind_left = X[:,splitting_feature] < splitting_thres\n",
    "        left_split = X[ind_left,:]\n",
    "        y_left = y[ind_left]\n",
    "            \n",
    "        ind_right = X[:,splitting_feature] >= splitting_thres\n",
    "        right_split = X[ind_right,:]\n",
    "        y_right = y[ind_right]\n",
    "        \n",
    "    except IndexError: \n",
    "        print Xcurrent.shape\n",
    "        print range(Xcurrent.shape[1])\n",
    "        print ind_left.shape\n",
    "        print splitting_feature\n",
    "        print splitting_thres\n",
    "\n",
    "    if Verbose == True:\n",
    "        print \"Split on feature %s. (%s, %s), Threshold = %s\" % (\\\n",
    "        splitting_feature, y_left.shape, y_right.shape, splitting_thres)\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if y_left.shape == y.shape[0]:\n",
    "        #print \"Creating leaf node.\"\n",
    "        return create_leaf(y_left,C)\n",
    "    if y_right.shape == y.shape[0]:\n",
    "        #print \"Creating leaf node.\"\n",
    "        return create_leaf(y_right,C)\n",
    "        \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = decision_tree_create(left_split, y_left, N_features_to_sample, C, min_node_size, Nbins, current_depth + 1, max_depth)        \n",
    "    right_tree = decision_tree_create(right_split, y_right, N_features_to_sample, C, min_node_size, Nbins, current_depth + 1, max_depth)\n",
    "\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'threshold'        : splitting_thres,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree,\n",
    "            'labels_distribution': None \n",
    "            \n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a recursive function to count the nodes and leaves in your tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_leaves(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1 \n",
    "    return count_leaves(tree['left']) + count_leaves(tree['right'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create and train a new forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forest_create(X,y,ntrees,nvarsample=None, min_node_size = 5, Nbins = 10):\n",
    "    \n",
    "    if nvarsample == None:\n",
    "        nvarsample = (np.round(np.sqrt(X.shape[1]))).astype(int)\n",
    "        print 'Nfeatures = %s'%nvarsample\n",
    "    \n",
    "    #the number of classes is inferred from the data\n",
    "    C = len(np.unique(y))\n",
    "    \n",
    "    nptrain = X.shape[0] #how many datapoints each tree is trained (same size of X)\n",
    "    RF = []\n",
    "    #for loop creating and training each tree \n",
    "    #bootstrap X to train each tree\n",
    "    for t in range(ntrees):\n",
    "        print 'current trained tree = %s'%t\n",
    "        #create bootstrap training dataset for tree t\n",
    "        indbootstrap = np.random.choice(X.shape[0],nptrain)\n",
    "        Xtree = X[indbootstrap,:]\n",
    "        ytree = y[indbootstrap] \n",
    "        \n",
    "        #train the tree\n",
    "        tree1 = decision_tree_create(Xtree,ytree,nvarsample,C,min_node_size,Nbins,max_depth=20,Verbose=True)\n",
    "        RF.append(tree1)\n",
    "    \n",
    "    print 'Forest Trained!'\n",
    "    return RF\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nfeatures = 3\n",
      "current trained tree = 0\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (1000 data points).\n",
      "Features selected = [5 1 9]\n",
      "Split on feature 1. ((320,), (680,)), Threshold = 0.313990808468\n",
      "current trained tree = 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (1000 data points).\n",
      "Features selected = [0 4 5]\n",
      "Split on feature 0. ((730,), (270,)), Threshold = 0.719011911974\n",
      "current trained tree = 2\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (1000 data points).\n",
      "Features selected = [4 5 8]\n",
      "Split on feature 4. ((989,), (11,)), Threshold = 0.99562301734\n",
      "current trained tree = 3\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (1000 data points).\n",
      "Features selected = [3 8 4]\n",
      "Split on feature 8. ((940,), (60,)), Threshold = 0.943912072456\n",
      "current trained tree = 4\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (1000 data points).\n",
      "Features selected = [2 4 0]\n",
      "Split on feature 0. ((430,), (570,)), Threshold = 0.401255673888\n",
      "current trained tree = 5\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (1000 data points).\n",
      "Features selected = [1 3 7]\n",
      "Split on feature 1. ((450,), (550,)), Threshold = 0.460973370378\n",
      "current trained tree = 6\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (1000 data points).\n",
      "Features selected = [8 0 3]\n",
      "Split on feature 0. ((639,), (361,)), Threshold = 0.660195518278\n",
      "current trained tree = 7\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (1000 data points).\n",
      "Features selected = [4 7 1]\n",
      "Split on feature 1. ((279,), (721,)), Threshold = 0.282618701251\n",
      "current trained tree = 8\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (1000 data points).\n",
      "Features selected = [0 7 9]\n",
      "Split on feature 0. ((690,), (310,)), Threshold = 0.710715880567\n",
      "current trained tree = 9\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (1000 data points).\n",
      "Features selected = [1 7 3]\n",
      "Split on feature 1. ((470,), (530,)), Threshold = 0.451552977895\n",
      "Forest Trained!\n"
     ]
    }
   ],
   "source": [
    "RF = forest_create(Xtrain,y,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(tree, x):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        return tree['labels_distribution'] \n",
    "    else:\n",
    "        # split on feature.\n",
    "        val_split_feature = x[tree['splitting_feature']]\n",
    "        if val_split_feature < tree['threshold']:\n",
    "            return classify(tree['left'], x)\n",
    "        else:\n",
    "            return classify(tree['right'],x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#outputs the posterior prob of each tree and the corresponding class\n",
    "def forest_posterior(RF,x):\n",
    "\n",
    "    T = len(RF)  #the number of trees \n",
    "\n",
    "    #infer the number of classes\n",
    "    P0 = classify(RF[0],x)\n",
    "    C = len(P0)\n",
    "    \n",
    "    Pt = np.zeros((T,C)) #matrix of posteriors from each tree (T x Nclasses)\n",
    "    Pt[0,:] = P0\n",
    "    for t in range(len(RF))[1:]:\n",
    "        Pt[t,:] = classify(RF[t],x) \n",
    "    return Pt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#classify input based on majority voting of each tree prediction\n",
    "def forest_classify_majority(RF,x):\n",
    "        Pt = forest_posterior(RF,x)\n",
    "        Yt = np.argmax(Pt,axis=1)         \n",
    "        C,unique_counts = np.unique(Yt,return_counts=True) #the id of classes and number of each\n",
    "        return C[np.argmax(unique_counts)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#classify input by averaging posteriors \n",
    "def forest_classify_ensemble(RF,x):\n",
    "    Pt = forest_posterior(RF,x)\n",
    "    Pforest = Pt.mean(axis=0)\n",
    "    ypred = np.argmax(Pt.mean(axis=0))\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_error(RF, X, y, method = None):\n",
    "    \n",
    "    # Apply the forest_classify(RF, x) to each row in your data\n",
    "    if method == None:\n",
    "        ypred = map(lambda x: forest_classify_ensemble(RF,x), X)\n",
    "        # Once you've made the predictions, calculate the classification error and return it\n",
    "        mistakes = sum(ypred != y)\n",
    "        error = mistakes/len(y)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(RF,Xtrain,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   1.   0. ]\n",
      " [ 0.   1.   0. ]\n",
      " [ 0.   1.   0. ]\n",
      " [ 0.   1.   0. ]\n",
      " [ 0.   1.   0. ]\n",
      " [ 0.   1.   0. ]\n",
      " [ 0.5  0.   0.5]\n",
      " [ 0.   1.   0. ]\n",
      " [ 0.   1.   0. ]\n",
      " [ 0.   0.   1. ]]\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "x= Xtrain[101,:]\n",
    "print forest_posterior(RF,x)\n",
    "print y[101]\n",
    "\n",
    "print forest_classify_majority(RF,x)\n",
    "print forest_classify_ensemble(RF,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Random Forest on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../MNIST_data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10cd56a10>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD8CAYAAABTq8lnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b418f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xtest = mnist.test.images[1,:]\n",
    "xtest = xtest.reshape(28,28)\n",
    "plt.imshow(xtest,'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(10000, 784)\n",
      "(55000,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "xtrain = mnist.train.images\n",
    "xtest = mnist.test.images\n",
    "ytrain = mnist.train.labels\n",
    "ytest = mnist.test.labels\n",
    "print xtrain.shape\n",
    "print xtest.shape\n",
    "print ytrain.shape\n",
    "print np.unique(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nfeatures = 28\n",
      "current trained tree = 0\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (10000 data points).\n",
      "Features selected = [ 81 734 589 187 509 322 441 377  74 659 332 237 173 331 204 324 143 492\n",
      " 766 679 104 732 776 465 630 229 636 189]\n",
      "Split on feature 377. ((4573,), (5427,)), Threshold = 0.0235294\n",
      "current trained tree = 1\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (10000 data points).\n",
      "Features selected = [593 592 521 451  14 269 745  27 444  38 135 386 105 394  51 514  64 605\n",
      " 406 404 223 776 711 194   9   5 470 511]\n",
      "Split on feature 406. ((4389,), (5611,)), Threshold = 0.376471\n",
      "(20, 28)\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "(20, 1, 28)\n",
      "[[367 388 661 712 426 759 604 200  86  85 713 450   7  75 220 282 116 219\n",
      "  536 770 218 778 530 696   6 665 528 168]]\n",
      "None\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'y_left' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-5739bf00111c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRF_MNIST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforest_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-144-b284ae055303>\u001b[0m in \u001b[0;36mforest_create\u001b[0;34m(X, y, ntrees, nvarsample, min_node_size, Nbins)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#train the tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtree1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnvarsample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVerbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mRF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-9415b05722c4>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, C, min_node_size, Nbins, current_depth, max_depth, Verbose)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-9415b05722c4>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, C, min_node_size, Nbins, current_depth, max_depth, Verbose)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     return {'is_leaf'          : False, \n",
      "\u001b[0;32m<ipython-input-138-9415b05722c4>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, C, min_node_size, Nbins, current_depth, max_depth, Verbose)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-9415b05722c4>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, C, min_node_size, Nbins, current_depth, max_depth, Verbose)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     return {'is_leaf'          : False, \n",
      "\u001b[0;32m<ipython-input-138-9415b05722c4>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, C, min_node_size, Nbins, current_depth, max_depth, Verbose)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     return {'is_leaf'          : False, \n",
      "\u001b[0;32m<ipython-input-138-9415b05722c4>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, C, min_node_size, Nbins, current_depth, max_depth, Verbose)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-9415b05722c4>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, C, min_node_size, Nbins, current_depth, max_depth, Verbose)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-9415b05722c4>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, C, min_node_size, Nbins, current_depth, max_depth, Verbose)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Repeat (recurse) on left and right subtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mleft_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mright_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_features_to_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_node_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-9415b05722c4>\u001b[0m in \u001b[0;36mdecision_tree_create\u001b[0;34m(X, y, N_features_to_sample, C, min_node_size, Nbins, current_depth, max_depth, Verbose)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Create a leaf node if the split is \"perfect\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;31m#print \"Creating leaf node.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_leaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'y_left' referenced before assignment"
     ]
    }
   ],
   "source": [
    "RF_MNIST = forest_create(xtest,ytest,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([1,2])<2\n",
    "print b\n",
    "print a[b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute the path to the leaf followed by data point x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datapath(tree, x, branch = 1):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        return branch \n",
    "    else:\n",
    "        # split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "\n",
    "        if split_feature_value == 0:\n",
    "            \n",
    "            return datapath(tree['left'], x, 2*branch)\n",
    "        else:\n",
    "            return datapath(tree['right'],x, 2*branch+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expansion/Reduction functions for the transfer forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Expansion/Reduction of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expansion_reduction(tree,dataT1,dataT2,features,target):\n",
    "\n",
    "    Tree = tree #a copy of the tree\n",
    "    \n",
    "    leavesData1 = np.zeros(len(dataT1))\n",
    "    leavesData2 = np.zeros(len(dataT2))\n",
    "    \n",
    "    for i in range(len(dataT1)):\n",
    "        leavesData1[i]=  datapath(tree,dataT1[i]) #indicates the leaf where each data point ends up\n",
    "    for i in range(len(dataT2)):\n",
    "        leavesData2[i]=  datapath(tree,dataT2[i]) #indicates the leaf where each data point ends up\n",
    "        \n",
    "    Uleaves1 = np.unique(leavesData1)  #the path to each leaf followed by data1\n",
    "    Uleaves2 = np.unique(leavesData2)  #the path to each leaf followed by data2\n",
    "    Uleaves = list(set(Uleaves1) & set(Uleaves2)) #leaves reached by both data1 and data2\n",
    "            \n",
    "    #expanding each leaf on the 1st bootstrap replica of target data\n",
    "    for i in Uleaves:\n",
    "        ind_data1 = np.where(leavesData1==i) #indices of datapoints for each leaf\n",
    "        print ind_data1\n",
    "        Exp_tree = decision_tree_create(dataT1[ind_data1], features, 'safe_loans', max_depth = 3)\n",
    "\n",
    "        #classification error at each leaf for Data T2\n",
    "        ind_data2 = np.where(leavesData2==i)\n",
    "        Err_leavesT2 = intermediate_node_num_mistakes(dataT2[ind_data2][target])/len(dataT2[ind_data2])\n",
    "\n",
    "        #error at the current subtree on Data T2\n",
    "        Err_subtreeT2 = evaluate_classification_error(Exp_tree, dataT2[ind_data2])\n",
    "        \n",
    "        #comparing the error of the subtree with that at the leaf node of the original tree\n",
    "        if Err_subtreeT2 < Err_leavesT2:\n",
    "            Tree = mergetrees(Tree,i,Exp_tree)\n",
    "    \n",
    "    return Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergetrees(tree1,leafnr,tree2):\n",
    "    leafnrbin = bin(leafnr)[3:]\n",
    "    path = ''\n",
    "    for i in range(len(leafnrbin)):\n",
    "        if leafnrbin[i] == '0':\n",
    "            path=path+str(\"['left']\")\n",
    "        else:\n",
    "            path=path+str(\"['right']\") \n",
    "        print(path)\n",
    "    exec ('tree1'+path+\"['prediction']\"+'=None')\n",
    "    exec ('tree1'+path+\"['is_leaf']\"+'=False')\n",
    "    exec ('tree1'+path+\"['left']\"+\"=tree2['left']\")\n",
    "    exec ('tree1'+path+\"['right']\"+\"=tree2['right']\")\n",
    "    exec ('tree1'+path+\"['splitting_feature']\"+\"=tree2['splitting_feature']\")\n",
    "    \n",
    "#    print ('tree1'+path+\"['prediction']\"+'=None')\n",
    "#    print ('tree1'+path+\"['is_leaf']\"+'=False')\n",
    "#    print ('tree1'+path+\"['left']\"+\"=tree2['left']\")\n",
    "#    print ('tree1'+path+\"['right']\"+\"=tree2['right']\")\n",
    "#    print ('tree1'+path+\"['splitting_feature']\"+\"=tree2['splitting_feature']\")\n",
    "    \n",
    "    #print('tree1'+path+'=tree2')\n",
    "    return tree1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing out a decision stump (To be updated to the numpy version!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stump(tree, name = 'root'):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print \"(leaf, label: %s)\" % tree['prediction']\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('.')\n",
    "    print '                       %s' % name\n",
    "    print '         |---------------|----------------|'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '  [{0} == 0]               [{0} == 1]    '.format(split_name)\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '    (%s)                         (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term. 36 months == 0]               [term. 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the intermediate left subtree\n",
    "\n",
    "The tree is a recursive dictionary, so we do have access to all the nodes! We can use\n",
    "* `my_decision_tree['left']` to go left\n",
    "* `my_decision_tree['right']` to go right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       term. 36 months\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.A == 0]               [grade.A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left'], my_decision_tree['splitting_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the left subtree of the left subtree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       grade.A\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.B == 0]               [grade.B == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left']['left'], my_decision_tree['left']['splitting_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       term. 36 months\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.D == 0]               [grade.D == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (leaf, label: -1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['right'], my_decision_tree['splitting_feature'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
