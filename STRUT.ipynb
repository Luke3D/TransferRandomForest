{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  STRUT Without Swapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from math import log\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def convert_from_scikit_learn_to_dic_ite_strut(node_index,is_leaves, children_left,children_right,feature,threshold,value,labels,C):\n",
    "        \n",
    "        a = is_leaves[0]\n",
    "        b = feature[0]\n",
    "        c = threshold[0]\n",
    "        if (a):\n",
    "            d = value[0]  #datapoints of each class in the node\n",
    "            d2 = np.squeeze(d/np.sum(d))\n",
    "            d3 = np.zeros(C)\n",
    "            d3[labels] = d2\n",
    "            e = labels[np.argmax(d2)]\n",
    "            return {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True,\n",
    "            'prediction': e,\n",
    "            'labels_distribution':d3}\n",
    "    \n",
    "        else:\n",
    "            left = children_left[0]-node_index[0]\n",
    "            if(left==-1):\n",
    "                left_tree = None\n",
    "            else:\n",
    "                left_tree = convert_from_scikit_learn_to_dic_ite_strut(node_index[left:],is_leaves[left:], children_left[left:],children_right[left:],feature[left:],threshold[left:],value[left:],labels,C)\n",
    "            right = children_right[0]-node_index[0]\n",
    "            if(right==-1):\n",
    "                right_tree = None\n",
    "            else:\n",
    "                right_tree = convert_from_scikit_learn_to_dic_ite_strut(node_index[right:],is_leaves[right:], children_left[right:],children_right[right:],feature[right:],threshold[right:],value[right:],labels,C)\n",
    "            return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': b,\n",
    "            'threshold'        : c,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree,\n",
    "            'labels_distribution': None}\n",
    "\n",
    "def convert_from_scikit_learn_to_dic_strut(feature,threshold,C,Q,children_left,children_right):\n",
    "    # C is the size of the whole labels\n",
    "    # labels are the labels that are used in the this tree\n",
    "    labels = range(0,C,1)\n",
    "    n_nodes = len(children_left)\n",
    "    node_index = np.array(range(0,n_nodes))\n",
    "    Val = Q   #datapoints in node\n",
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "    node_depth = np.zeros(shape=n_nodes)\n",
    "    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "    stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "    while len(stack) > 0:\n",
    "        node_id, parent_depth = stack.pop()\n",
    "        node_depth[node_id] = parent_depth + 1\n",
    "    # If we have a test node\n",
    "        if (children_left[node_id] != children_right[node_id]):\n",
    "            stack.append((children_left[node_id], parent_depth + 1))\n",
    "            stack.append((children_right[node_id], parent_depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "            \n",
    "    return convert_from_scikit_learn_to_dic_ite_strut(node_index,is_leaves, children_left,children_right,feature,threshold,Val,labels,C)\n",
    "\n",
    "def treesubset(subset,children_left,children_right):\n",
    "    ch_left = np.zeros(len(children_left))\n",
    "    ch_right = np.zeros(len(children_right))  \n",
    "    for i in range(len(subset)):\n",
    "        (l,) = np.where(subset==children_left[i])\n",
    "        (r,) = np.where(subset==children_right[i])\n",
    "        if(l.shape[0]==0):\n",
    "            ch_left[i] = -1\n",
    "        else:\n",
    "            ch_left[i] = l\n",
    "            \n",
    "        if(r.shape[0]==0):\n",
    "            ch_right[i] = -1\n",
    "        else:\n",
    "            ch_right[i] = r        \n",
    "    return (ch_left,ch_right)\n",
    "\n",
    "def kl (p,q): # Kullback-libler divegence\n",
    "    p = np.asarray(p, dtype=np.float)\n",
    "    q = np.asarray(q, dtype=np.float)\n",
    "    return np.sum(np.where(p != 0,p * np.log10((p / q)), 0))\n",
    "\n",
    "def jsd(p,q): # Symmetric Kullback-libler divergence\n",
    "    p = np.asarray(p, dtype=np.float)\n",
    "    q = np.asarray(q, dtype=np.float)\n",
    "    m = (p+q)/2\n",
    "    return (kl(p,m)+kl(m,q))/2\n",
    "\n",
    "def infogain(yleft,len_left,yright,len_right):\n",
    "    yparent = (yleft+yright)/2\n",
    "    N = len_left+len_right\n",
    "    #compute information gain\n",
    "    I = entropy(yparent) -( (len_left/N)*entropy(yleft) + (len_right/N)*entropy(yright) )   \n",
    "    return I\n",
    "\n",
    "#entropy for multiple classes\n",
    "def entropy(y):\n",
    "    y1 = y[y!=0]\n",
    "    H = -(y1*np.log10(y1)).sum()\n",
    "    return H \n",
    "\n",
    "#computes split for given feature\n",
    "def partition(Xtarget,ytarget,index_of_data,feature,C,threshold): # divide the data to the left and rightbased on the threshold\n",
    "    left = index_of_data[Xtarget[index_of_data,feature]<threshold]\n",
    "    if(len(left)==0):\n",
    "        left = index_of_data[Xtarget[index_of_data,feature]<=threshold]\n",
    "    labels_left = ytarget[left]\n",
    "    qL = np.bincount(labels_left)\n",
    "    right = index_of_data[Xtarget[index_of_data,feature]>=threshold]\n",
    "    labels_right = ytarget[right]\n",
    "    qR = np.bincount(labels_right)\n",
    "    qR = np.append(qR,np.zeros(np.max([C-qR.shape[0],0])))\n",
    "    qL = np.append(qL,np.zeros(np.max([C-qL.shape[0],0]))) \n",
    "    qL = qL/qL.sum()\n",
    "    qR = qR/qR.sum()\n",
    "    return [qL,left,qR,right]\n",
    "\n",
    "def dg(Sleft,lenleft,Sright,lenright,QL,QR): # DG function as in the paper    \n",
    "    return 1-(lenleft/(lenleft+lenright))*jsd(Sleft,QL)-(lenright/(lenleft+lenright))*jsd(Sright,QR)\n",
    "    \n",
    "#optimize threshold of given node based on target data\n",
    "#INPUTS: Xtarget, ytarget, S:indices of datapoints that reach node, f:feature of node, QL,QR:Distributions from source reaching children\n",
    "#        C:# of classes, verbos: print additional info\n",
    "#OUTPUTS: [th, ql, qr, left, right]: th: new threshold; ql,qr: distribution from target with new threshold; left,right: target data indices\n",
    "#         going to left and right nodes\n",
    "#Depending fcns: partition(), df(), infogain(), jsd(), kl(), entropy()\n",
    "\n",
    "def threshold_selection(X,y,S,f,QL,QR,C,verbos): # finding the best threshold\n",
    "    fvals = np.sort(X[S,f])\n",
    "    num_data_points = len(fvals)\n",
    "    N = 50 # # of bins to search threshold\n",
    "    Val_DG  = np.array([]) #contains values of DG for each bin\n",
    "    Val_infogain = np.array([])\n",
    "    if num_data_points > N-1: \n",
    "        I = range(0,num_data_points,np.floor(num_data_points/N).astype(int))\n",
    "        fvals = fvals[I[1:-1]]\n",
    "    for i in fvals:\n",
    "        [qL, left, qR, right] = partition(X,y,S,f,C,i) #compute histogram (qL,qR) for left and right children with current threshold\n",
    "        #fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey='row')\n",
    "        #ax1.plot(QL)\n",
    "        #ax1.set_title('QL')\n",
    "        #ax2.plot(Sleft, color='r')\n",
    "        #ax2.set_title('QprimeL')\n",
    "        #ax3.plot(QR)\n",
    "        #ax3.set_title('QR')\n",
    "        #ax4.plot(Sright, color='r')\n",
    "        #ax4.set_title('QprimeR')\n",
    "        Val_DG = np.append(Val_DG,dg(qL,len(left),qR,len(right),QL,QR))\n",
    "        Val_infogain = np.append(Val_infogain,infogain(qL,len(left),qR,len(right)))        #Val_swap = np.append(Val_swap,dg(Sleft,len(left),Sright,len(right),QR,QL)) # this is the divergence measure for each threshold split  \n",
    "    if(verbos):\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharex='col', sharey='row')\n",
    "    #ax1.plot(Val)\n",
    "    #ax1.set_title('DG')\n",
    "    #ax2.plot(Val_infogain)\n",
    "    #ax2.set_title('infogain')\n",
    "        ax1.plot(fvals,Val,'r')\n",
    "        ax1.hold(True)\n",
    "        ax1.plot(fvals,Val_infogain)\n",
    "        ax1.hold(False)\n",
    "        ax1.set_title('DG and Infogain')\n",
    "    #plt.show()\n",
    "    #find nan and assign the minimum value found in the array to remove the nan \n",
    "    Val_DG[np.isnan(Val_DG)] = min(Val_DG[~np.isnan(Val_DG)])\n",
    "    Val_infogain[np.isnan(Val_infogain)] = min(Val_infogain[~np.isnan(Val_infogain)])\n",
    "    #Val_swap[np.isnan(Val_swap)] = min(Val_swap[~np.isnan(Val_swap)])\n",
    "    th_DG = fvals[np.argmax(Val_DG)]\n",
    "    th_infogain = fvals[np.argmax(Val_infogain)]\n",
    "    \n",
    "    #find new threshold based on how many datapoints we have in current node\n",
    "    if(len(S)<0):\n",
    "        [ql, left, qr, right] = partition(X,y,S,f,C,th_infogain)\n",
    "        th = th_infogain\n",
    "    else:\n",
    "        [ql, left, qr, right] = partition(X,y,S,f,C,th_DG)\n",
    "        th = th_DG\n",
    "        \n",
    "    if(verbos):\n",
    "        ax2.plot(ql)\n",
    "        ax2.hold(True)\n",
    "        ax2.plot(qr)\n",
    "        ax2.hold(False)\n",
    "        ax2.set_title('Dist Target Data')\n",
    "\n",
    "        ax3.plot(QL)\n",
    "        ax3.hold(True)\n",
    "        ax3.plot(QR)\n",
    "        ax3.hold(False)\n",
    "        ax3.set_title('Dist Source Data')\n",
    "        \n",
    "    return [th, ql, qr, left, right]\n",
    "\n",
    "def classify(tree, x):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        return tree['labels_distribution'] \n",
    "    else:\n",
    "        # split on feature.\n",
    "        val_split_feature = x[tree['splitting_feature']]\n",
    "        if val_split_feature < tree['threshold']:\n",
    "            return classify(tree['left'], x)\n",
    "        else:\n",
    "            return classify(tree['right'],x)\n",
    "\n",
    "def forest_posterior(RF,x):\n",
    "\n",
    "    T = len(RF)  #the number of trees \n",
    "\n",
    "    #infer the number of classes\n",
    "    P0 = classify(RF[0],x)\n",
    "    C = len(P0)\n",
    "    \n",
    "    Pt = np.zeros((T,C)) #matrix of posteriors from each tree (T x Nclasses)\n",
    "    Pt[0,:] = P0\n",
    "    for t in range(len(RF))[1:]:\n",
    "        Pt[t,:] = classify(RF[t],x) \n",
    "    return Pt\n",
    "\n",
    "#classify input based on majority voting of each tree prediction\n",
    "def forest_classify_majority(RF,x):\n",
    "        Pt = forest_posterior(RF,x)\n",
    "        Yt = np.argmax(Pt,axis=1)         \n",
    "        C,unique_counts = np.unique(Yt,return_counts=True) #the id of classes and number of each\n",
    "        return C[np.argmax(unique_counts)]   \n",
    "    \n",
    "#classify input by averaging posteriors \n",
    "def forest_classify_ensemble(RF,x):\n",
    "    Pt = forest_posterior(RF,x)\n",
    "    Pforest = Pt.mean(axis=0)\n",
    "    ypred = np.argmax(Pt.mean(axis=0))\n",
    "    return ypred\n",
    "\n",
    "def evaluate_classification_error(RF, X, y, method = None):  \n",
    "    # Apply the forest_classify(RF, x) to each row in your data\n",
    "    if method == None:\n",
    "        ypred = map(lambda x: forest_classify_ensemble(RF,x), X)\n",
    "        # Once you've made the predictions, calculate the classification error and return it\n",
    "        mistakes = sum(ypred != y)\n",
    "        error = mistakes/len(y)\n",
    "    return error\n",
    "\n",
    "#returning the histogram of the classes for each node in estimator. N is # of classes\n",
    "def value_for_all(estimator,N):\n",
    "    from scipy.sparse import csr_matrix\n",
    "    ch_left = estimator.tree_.children_left\n",
    "    ch_right = estimator.tree_.children_right\n",
    "    (cl,) = np.where(ch_left!=-1)\n",
    "    (cr,) = np.where(ch_right!=-1)\n",
    "    cap = estimator.tree_.capacity\n",
    "    dis_node = np.zeros((cap,estimator.tree_.n_classes))\n",
    "    A = np.zeros([cap,cap])\n",
    "    D = A\n",
    "    A = csr_matrix(A)\n",
    "    A[cl,ch_left[cl]] = 1\n",
    "    A[cr,ch_right[cr]] = 1\n",
    "    B = A\n",
    "    C = B\n",
    "    while(C.sum()!=0):\n",
    "        C = A*C\n",
    "        B = B + C\n",
    "    I,J = B.nonzero()\n",
    "    D[I,J] = 1\n",
    "    (I,) = np.where(ch_left==-1)\n",
    "    dis_node[I,:] = np.squeeze(estimator.tree_.value[I])\n",
    "    for i in I:\n",
    "        dis_node[i,:] = dis_node[i,:]/dis_node[i,:].sum()\n",
    "    (remain1,) = np.where(ch_left!=-1)\n",
    "    for i in remain1:\n",
    "        (I,) = np.where(D[i,:]==1)\n",
    "        dis_node[i,:] = np.sum(np.squeeze(estimator.tree_.value[I]),axis = 0)\n",
    "        dis_node[i,:] = dis_node[i,:]/dis_node[i,:].sum()\n",
    "    Dis_node = np.zeros((cap,N))\n",
    "    Dis_node[:,estimator.classes_.astype(int)] = dis_node\n",
    "    return Dis_node\n",
    "    \n",
    "def STRUT(Xsource,ysource,Xtarget,ytarget,n_trees,C,verbos = False):\n",
    "    # Assumption: ysource has all the labels of the problem \n",
    "    Estimator = RandomForestClassifier(random_state=0,n_estimators=n_trees)  # REMOVE Random_state for final release\n",
    "    Estimator = Estimator.fit(Xsource, ysource)\n",
    "    ypred = Estimator.predict(Xtarget)\n",
    "    print sum(ypred!=ytarget)/len(ytarget)\n",
    "    RF = []\n",
    "    # Looping through all the trees in the forest RF\n",
    "    for rf in range(Estimator.n_estimators):\n",
    "        estimator = Estimator.estimators_[rf]\n",
    "        \n",
    "        dis_node = value_for_all(estimator,C)  #Histogram of classes for source (row: node index, col: class)\n",
    "        LF = estimator.tree_.children_left  #indices of left nodes \n",
    "        LR = estimator.tree_.children_right #indices of right nodes\n",
    "        Features = estimator.tree_.feature\n",
    "        num_nodes = estimator.tree_.capacity\n",
    "        print num_nodes\n",
    "        P = list(np.zeros(num_nodes)) #maintain a list of the target data indices at each node (0 is root) - capacity is node_count\n",
    "        P[0] = range(len(ytarget))\n",
    "        Q = np.zeros((num_nodes,C)) #Histogram of target data indices at each node\n",
    "        Q[0,:] = dis_node[0,:]\n",
    "        thresh = np.zeros(num_nodes) #init vector with new threshold at each node\n",
    "        remain = [0]    #indexing the remaining nodes as we move from top-down\n",
    "        subset = []     #maintains a list of the nodes that are reached from target data\n",
    "        #looping through the nodes\n",
    "        while(len(remain)!=0):\n",
    "            i = remain[0]\n",
    "            subset.append(i)  #updating the list of nodes reached by target data with current node index\n",
    "            index_left = LF[i]\n",
    "            index_right = LR[i]\n",
    "            #check if node is leaf\n",
    "            if(index_left!=-1):\n",
    "                QL = dis_node[index_left,:]  #distribution of labels in children nodes\n",
    "                QR = dis_node[index_right,:]\n",
    "                f = Features[i]  #feature of parent node\n",
    "                [th, ql, qr, left, right] = threshold_selection(Xtarget,ytarget,np.array(P[i]),f,QL,QR,C,verbos)\n",
    "                thresh[i] = th\n",
    "                P[index_left] = left\n",
    "                P[index_right] = right\n",
    "                Q[index_left,:] = ql\n",
    "                Q[index_right,:] = qr\n",
    "                \n",
    "                #if no target datapoints reach either the left or right children we make the parent node a leaf\n",
    "                if(len(left)>0 and len(right)>0):\n",
    "                    remain = np.append(remain,index_left)\n",
    "                    remain = np.append(remain,index_right)\n",
    "            remain = remain[1:]\n",
    "\n",
    "        subset = np.sort(subset)\n",
    "        lf =  LF[subset]\n",
    "        lr =  LR[subset]\n",
    "        (ch_lf,ch_lr) = treesubset(subset,lf,lr)\n",
    "        print len(subset)\n",
    "        ST = convert_from_scikit_learn_to_dic_strut(Features[subset],thresh[subset],C,Q[subset,:],ch_lf.astype(int),ch_lr.astype(int))\n",
    "        RF.append(ST)\n",
    "    return RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.234618181818\n",
      "5599\n",
      "5599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RoozbehFarhoudi/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:257: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47090909090909089"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF = STRUT(Xsource,ysource,Xtarget,ytarget,n_trees=1,C=10,verbos = False)\n",
    "evaluate_classification_error(RF, Xsource, ysource, method = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784)\n",
      "(10000, 784)\n",
      "(27500, 784)\n",
      "(27500, 784)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../MNIST_data/\", one_hot=False)\n",
    "Xtrain = mnist.train.images\n",
    "Xtest = mnist.test.images\n",
    "ytrain = mnist.train.labels\n",
    "ytest = mnist.test.labels\n",
    "Xsource, Xtarget, ysource, ytarget = train_test_split(Xtrain, ytrain, test_size=0.5)\n",
    "print Xtrain.shape\n",
    "print Xtest.shape\n",
    "print Xsource.shape\n",
    "print Xtarget.shape\n",
    "XT1,XT2,yT1,yT2 = train_test_split(Xtarget,ytarget,test_size=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
